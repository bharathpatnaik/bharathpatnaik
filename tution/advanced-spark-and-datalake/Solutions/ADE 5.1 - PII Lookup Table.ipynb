{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Creating a Pseudonymized PII Lookup Table\n",
    "\n",
    "In this lesson we'll create a pseudonymized key for storing potentially sensitive user data.\n",
    "\n",
    "Our approach in this notebook is fairly straightforward; some industries may require more elaborate de-identification to guarantee privacy.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/ade/ADE_arch_user_lookup.png\" width=\"60%\" />\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lesson, students will be able to:\n",
    "- Describe the purpose of \"salting\" before hashing\n",
    "- Apply salted hashing to sensitive data for pseudonymization\n",
    "- Use Auto Loader to process incremental inserts"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "8c0bb7f4-51a6-47c6-ba3a-10a72aa03a44",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "Begin by running the following cell to set up relevant databases and paths."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "4d5ab3b0-b7fd-451b-85ef-c75169e0d94b",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%run ../Includes/Classroom-Setup-6.1"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "085703f6-f3f9-4406-a557-049965d59c19",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Auto Load Bronze Data\n",
    "\n",
    "The following cell defines and executes logic to incrementally ingest data into the **`registered_users`** table with Auto Loader.\n",
    "\n",
    "This logic is currently configured to process a single file each time a batch is triggered (currently every 10 seconds).\n",
    "\n",
    "Executing this cell will start an always-on stream that slowly ingests new files as they arrive."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "52d150a4-7651-439d-afbe-3f168b6b2086",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "query = (spark.readStream\n",
    "              .schema(\"device_id LONG, mac_address STRING, registration_timestamp DOUBLE, user_id LONG\")\n",
    "              .format(\"cloudFiles\")\n",
    "              .option(\"cloudFiles.format\", \"json\")\n",
    "              .option(\"cloudFiles.maxFilesPerTrigger\", 1)\n",
    "              .load(DA.paths.raw_user_reg)\n",
    "              .writeStream\n",
    "              .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/registered_users\")\n",
    "              .trigger(processingTime=\"10 seconds\")\n",
    "              .table(\"registered_users\"))\n",
    "\n",
    "DA.block_until_stream_is_ready(query)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "aa8b2e01-6260-409f-ab39-7dd55ede633e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before moving on with this lesson, we need to:\n",
    "1. Stop the existing stream\n",
    "2. Drop the table we created\n",
    "3. Clear the checkpoint directory"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "ad21520e-a7a6-468a-8b50-79e5652b042e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "query.stop()\n",
    "query.awaitTermination()\n",
    "spark.sql(\"DROP TABLE IF EXISTS registered_users\")\n",
    "dbutils.fs.rm(f\"{DA.paths.checkpoints}/registered_users\", True)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "b906f9b6-b660-4f2f-9e29-4818816aff5a",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the cell below to refactor the above query into a function that processes new files as a single incremental triggered batch.\n",
    "\n",
    "To do this:\n",
    "* Remove the option limiting the amount of files processed per trigger (this is ignored when executing a batch anyway)\n",
    "* Change the trigger type to \"availableNow\"\n",
    "* Make sure to add **`.awaitTermination()`** to the end of your query to block execution of the next cell until the batch has completed"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "aebe9069-f04a-4678-bd30-8aec7679b7e1",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ANSWER\n",
    "def ingest_user_reg():\n",
    "    query = (spark.readStream\n",
    "                  .schema(\"device_id LONG, mac_address STRING, registration_timestamp DOUBLE, user_id LONG\")\n",
    "                  .format(\"cloudFiles\")\n",
    "                  .option(\"cloudFiles.format\", \"json\")\n",
    "                  .load(DA.paths.raw_user_reg)\n",
    "                  .writeStream\n",
    "                  .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/registered_users\")\n",
    "                  .trigger(availableNow=True)\n",
    "                  .option(\"path\", f\"{DA.paths.user_db}/registered_users\")\n",
    "                  .table(\"registered_users\"))\n",
    "    \n",
    "    query.awaitTermination()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "898c5285-3c2b-48bc-b981-f83ff6730c7d",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use your function below.\n",
    "\n",
    "**NOTE**: This triggered batch will automatically cause our always-on stream to fail because the same checkpoint is used; default behavior will allow the newer query to succeed and error the older query."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "f2794b2f-1ee7-4daf-9d3a-3f5be2b306fb",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ingest_user_reg()\n",
    "display(spark.table(\"registered_users\"))"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "4efda1d8-2dc3-4b98-b95f-5a3e981c17b9",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another custom class was initialized in the setup script to land a new batch of data in our source directory. \n",
    "\n",
    "Execute the following cell and note the difference in the total number of rows in our tables."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "a284b0a8-078b-4612-91b6-4d46a9e8605e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "DA.user_reg_stream.load()\n",
    "\n",
    "ingest_user_reg()\n",
    "display(spark.table(\"registered_users\"))"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "5337cba9-d9cc-406b-94c2-6d8425ee7f05",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add a Salt to Natural Key\n",
    "We'll start by defining a salt, here in plain text. We'll combine this salt with our natural key, **`user_id`**, before applying a hash function to generate a pseudonymized key.\n",
    "\n",
    "Salting before hashing is very important as it makes dictionary attacks to reverse the hash much more expensive.  To demonstrate, try reversing the following SHA-256 hash of `Secrets123` by searching it's hash using Google: `FCF730B6D95236ECD3C9FC2D92D7B6B2BB061514961AEC041D6C7A7192F592E4` bringing you to this [link](https://hashtoolkit.com/decrypt-sha256-hash/fcf730b6d95236ecd3c9fc2d92d7b6b2bb061514961aec041d6c7a7192f592e4).  Next, try hashing `Secrets123:BEANS` [here](https://passwordsgenerator.net/sha256-hash-generator/) and perform a similar search.  Notice how adding the salt `BEANS` improved the security.\n",
    "\n",
    "For greater security, we could upload the salt as a secret using the Databricks <a href=\"https://docs.databricks.com/security/secrets/secrets.html\" target=\"_blank\">Secrets</a> utility."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "46b85b17-4350-4672-be00-51d502d8b89f",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "salt = 'BEANS'\n",
    "spark.conf.set(\"da.salt\", salt)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "ed2dbbd2-da5c-4eed-8c2c-ef4f0319b8d5",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "# # If using the Databricks secrets store, here's how you'd read it...\n",
    "# salt = dbutils.secrets.get(scope=\"DA-ADE3.03\", key=\"salt\")\n",
    "# salt"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "95bc9a06-ece8-4c81-90df-a7edb372aaf6",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preview what your new key will look like."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "eeca686f-42a8-4aa5-8c5c-42b9ef07c6ae",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT *, sha2(concat(user_id,\"${da.salt}\"), 256) AS alt_id\n",
    "FROM registered_users"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "147b1b22-abf2-4280-8c28-ca7285725398",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Register a SQL UDF\n",
    "\n",
    "Create a SQL user-defined function to register this logic to the current database under the name **`salted_hash`**. \n",
    "\n",
    "This will allow this logic to be called by any user with appropriate permissions on this function. \n",
    "\n",
    "Make sure your UDF accepts one parameter: a **`String`** and it should return a **`STRING`**.  You can access the configured salt value by using the expression `\"${da.salt}\"`.\n",
    "\n",
    "For more information, see the <a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/sql-ref-syntax-ddl-create-sql-function.html\" target=\"_blank\">CREATE FUNCTION</a> method from the SQL UDFs docs."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "d2f72227-1d1c-428b-9625-b9dc245f4c23",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "-- ANSWER\n",
    "CREATE OR REPLACE FUNCTION salted_hash (id STRING) RETURNS STRING\n",
    "RETURN sha2(concat(id,\"${da.salt}\"), 256)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "4539638a-b8e4-4c9b-897a-9710e8523b04",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "If your SQL UDF is defined correctly, the assert statement below should run without error."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "b16d2a32-23bf-4704-93d7-00f4e24901ec",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Check your work\n",
    "set_a = spark.sql(f\"SELECT sha2(concat('USER123', '{salt}'), 256) alt_id\").collect()\n",
    "set_b = spark.sql(\"SELECT salted_hash('USER123') alt_id\").collect()\n",
    "assert set_a == set_b, \"The 'salted_hash' function is returning the wrong result.\"\n",
    "print(\"All tests passed.\")"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "98bce950-48a2-46c9-99ea-d6449289922e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that it is theoretically possible to link the original key and pseudo-ID if the hash function and the salt are known.\n",
    "\n",
    "Here, we use this method to add a layer of obfuscation; in production, you may wish to have a much more sophisticated hashing method."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "ddacd028-29ad-4c12-9285-d1938ca32107",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Register Target Table\n",
    "The logic below creates the **`user_lookup`** table.\n",
    "\n",
    "Here we're just creating our **`user_lookup`** table. In the next notebook, we'll use this pseudo-ID as the sole link to user PII.\n",
    "\n",
    "By controlling access to the link between our **`alt_id`** and other natural keys, we'll be able to prevent linking PII to other user data throughout our system."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "8f7a1eb8-0002-48a6-9132-ae68c81eaab4",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS user_lookup (alt_id string, device_id long, mac_address string, user_id long)\n",
    "USING DELTA \n",
    "LOCATION '${da.paths.working_dir}/user_lookup'"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "c87c8f66-02c3-4cdb-852a-fb67af27a7c5",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define a Function for Processing Incremental Batches\n",
    "\n",
    "The cell below includes the setting for the correct checkpoint path.\n",
    "\n",
    "Define a function to apply the SQL UDF registered above to create your **`alt_id`** to the **`user_id`** from the **`registered_users`** table.\n",
    "\n",
    "Make sure you include all the necessary columns for the target **`user_lookup`** table. Configure your logic to execute as a triggered incremental batch."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "d28b258c-6b29-4f13-b940-e5e91807e97f",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ANSWER\n",
    "def load_user_lookup():\n",
    "    query = (spark.readStream\n",
    "                  .table(\"registered_users\")\n",
    "                  .selectExpr(\"salted_hash(user_id) AS alt_id\", \"device_id\", \"mac_address\", \"user_id\")\n",
    "                  .writeStream\n",
    "                  .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/user_lookup\")\n",
    "                  .trigger(availableNow=True)\n",
    "                  .table(\"user_lookup\"))\n",
    "    \n",
    "    query.awaitTermination()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "ced8cbc3-8647-4899-baad-8d1e34969b59",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use your method below and display the results."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "4d694851-d865-4f95-a1bb-18a2ce2865d2",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "load_user_lookup()\n",
    "\n",
    "display(spark.table(\"user_lookup\"))"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "c9b89041-ca36-4480-b669-f896bf048718",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Process another batch of data below to confirm that the incremental processing is working through the entire pipeline."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "1100ff91-084b-4652-b0f6-24083a131767",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "DA.user_reg_stream.load()\n",
    "\n",
    "ingest_user_reg()\n",
    "load_user_lookup()\n",
    "\n",
    "display(spark.table(\"user_lookup\"))"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "fd2ac88e-577e-44ee-8718-7c96e8a4624a",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "The code below ingests all the remaining records to put 100 total users in the **`user_lookup`** table."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "cd6a24b6-01ff-4fb9-8bf7-f3124bb0ee8c",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "DA.user_reg_stream.load(continuous=True)\n",
    "\n",
    "ingest_user_reg()\n",
    "load_user_lookup()\n",
    "\n",
    "display(spark.table(\"user_lookup\"))"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "e93877b3-8d0a-492b-b376-d2bbac3fb1b8",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll apply this same hashing method to process and store PII data in the next lesson."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "0440dba5-3b8b-4fee-8924-6de180984bd5",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the following cell to delete the tables and files associated with this lesson."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "c0ca3c40-203e-4819-bac4-7224847dffae",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "DA.cleanup()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "4513c96e-49ab-4a35-9135-131a8aefe598",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "notebookName": "ADE 5.1 - PII Lookup Table",
   "dashboards": [],
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "language": "python",
   "widgets": {},
   "notebookOrigID": 4185566132528553
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}