{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Auto Load Data to Multiplex Bronze\n",
    "\n",
    "The chief architect has decided that rather than connecting directly to Kafka, a source system will send raw records as JSON files to cloud object storage. In this notebook, you'll build a multiplex table that will ingest these records with Auto Loader and store the entire history of this incremental feed. The initial table will store data from all of our topics and have the following schema. \n",
    "\n",
    "| Field | Type |\n",
    "| --- | --- |\n",
    "| key | BINARY |\n",
    "| value | BINARY |\n",
    "| topic | STRING |\n",
    "| partition | LONG |\n",
    "| offset | LONG\n",
    "| timestamp | LONG |\n",
    "| date | DATE |\n",
    "| week_part | STRING |\n",
    "\n",
    "This single table will drive the majority of the data through the target architecture, feeding three interdependent data pipelines.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/ade/ADE_arch_bronze.png\" width=\"60%\" />\n",
    "\n",
    "**NOTE**: Details on additional configurations for connecting to Kafka are available <a href=\"https://docs.databricks.com/spark/latest/structured-streaming/kafka.html\" target=\"_blank\">here</a>.\n",
    "\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lesson, you should be able to:\n",
    "- Describe a multiplex design\n",
    "- Apply Auto Loader to incrementally process records\n",
    "- Configure trigger intervals\n",
    "- Use \"trigger-available-now\" logic to execute triggered incremental loading of data."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "04305901-ca01-4190-8fb6-aa7423534ae9",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell declares the paths needed throughout this notebook."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "ada6cab5-0d16-4492-9cb1-74c2138356be",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%run ../Includes/Classroom-Setup-3.1"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "fe671c8e-4181-4f31-a98b-2bdee6b9def3",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"https://files.training.databricks.com/images/icon_warn_24.png\"> All records are being stored on the DBFS root for this training example.\n",
    "\n",
    "Setting up separate databases and storage accounts for different layers\n",
    "of data is preferred in both development and production environments."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "610c00eb-058f-4af1-9c02-594d6fecb94c",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Examine Source Data\n",
    "\n",
    "Data files are being written to the path specified by the variable below.\n",
    "\n",
    "Run the following cell to examine the schema in the source data and determine if anything needs to be changed as it's being ingested."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "65f06062-7592-459c-a814-2198aa62549e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "spark.read.json(DA.paths.source_daily).printSchema()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "ca2b1728-21b3-4fc3-8e9d-b708be42991b",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare Data to Join with Date Lookup Table\n",
    "The initialization script has loaded a **`date_lookup`** table. This table has a number of pre-computed date values. Note that additional fields indicating holidays or financial quarters might often be added to this table for later data enrichment.\n",
    "\n",
    "Pre-computing and storing these values is especially important based on our desire to partition our data by year and week, using the string pattern **`YYYY-WW`**. While Spark has both **`year`** and **`weekofyear`** functions built in, the **`weekofyear`** function may not provide expected behavior for dates falling in the last week of December or <a href=\"https://spark.apache.org/docs/2.3.0/api/sql/#weekofyear\" target=\"_blank\">first week of January</a>, as it defines week 1 as the first week with >3 days.\n",
    "\n",
    "While this edge case is esoteric to Spark, a **`date_lookup`** table that will be used across the organization is important for making sure that data is consistently enriched with date-related details."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "863d405c-df28-4d76-8b54-aca9b902b880",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "\n",
    "DESCRIBE date_lookup"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "6f05713b-76c7-49b4-9713-386a2a898410",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "The current table being implemented requires that we capture the accurate **`week_part`** for each **`date`**.\n",
    "\n",
    "The following call creates the **`DataFrame`** needed for the subsequent join operation."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "71f68bff-98fd-4cdd-8bea-e5146785af8f",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "date_lookup_df = spark.table(\"date_lookup\").select(\"date\", \"week_part\")"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "671a13df-3a16-428d-b587-5f31baadb9ce",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Working with the JSON data stored in the **`DA.paths.source_daily`** location, transform the **`timestamp`** column as necessary to join it with the **`date`** column."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "114dd41a-b479-4864-807c-c2ad89436f25",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ANSWER\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "json_df = spark.read.json(DA.paths.source_daily)\n",
    "\n",
    "joined_df = (json_df.join(F.broadcast(date_lookup_df),\n",
    "                          F.to_date((F.col(\"timestamp\")/1000).cast(\"timestamp\")) == F.col(\"date\"),\n",
    "                          \"left\"))\n",
    "display(joined_df)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "d2bbe891-1d18-4c42-a44e-c72109d565d5",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Triggered Incremental Auto Loading to Multiplex Bronze Table\n",
    "\n",
    "Below is starter code for a function to incrementally process data from the source directory to the bronze table, creating the table during the initial write.\n",
    "\n",
    "Fill in the missing code to:\n",
    "- Configure the stream to use Auto Loader\n",
    "- Configure Auto Loader to use the JSON format\n",
    "- Perform a broadcast join with the date_lookup table\n",
    "- Partition the data by the **`topic`** and **`week_part`** fields"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "02a58a1b-7779-4a70-8824-f5e323dc729c",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ANSWER\n",
    "def process_bronze():\n",
    "    query = (spark.readStream\n",
    "                  .format(\"cloudFiles\")\n",
    "                  .option(\"cloudFiles.format\", \"json\")\n",
    "                  .option(\"cloudFiles.schemaLocation\", f\"{DA.paths.checkpoints}/bronze_schema\")\n",
    "                  .load(DA.paths.source_daily)\n",
    "                  .join(F.broadcast(date_lookup_df), F.to_date((F.col(\"timestamp\")/1000).cast(\"timestamp\")) == F.col(\"date\"), \"left\")\n",
    "                  .writeStream\n",
    "                  .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/bronze\")\n",
    "                  .partitionBy(\"topic\", \"week_part\")\n",
    "                  .trigger(availableNow=True)\n",
    "                  .table(\"bronze\"))\n",
    "    \n",
    "    query.awaitTermination()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "ea02701a-f64e-48f6-abc3-85767f9429ec",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the cell below to process an incremental batch of data."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "bb678038-4735-4cf1-ad28-a9612d968791",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "process_bronze()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "1795308c-3fc7-4ef4-b550-8c305749a137",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Review the count of processed records."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "02a6ed23-52ec-43bd-bffb-fc6f88edb62e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT COUNT(*) FROM bronze"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "0b00b10b-c151-4516-9303-494f01a952f6",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preview the data to ensure records are being ingested correctly."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "143e87ed-17b4-4e5a-9d56-5f11f957c793",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT * FROM bronze"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "ce3969ba-dcd8-4e6f-be7e-00e7985e60f5",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "The **`DA.daily_stream.load()`** code below is a helper class to land new data in the source directory.\n",
    "\n",
    "Executing the following cell should successfully process a new batch."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "522d4c4a-250f-4493-8bb9-f03a632b0956",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "DA.daily_stream.load()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "f96f9f00-50b5-4aef-bf23-6c2d735cbf8c",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "process_bronze()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "d264019d-4788-4fc5-bd22-cdbec2bd4b58",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Confirm the count is now higher."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "671b3acf-9dd1-434f-ab72-919a801415b9",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT COUNT(*) FROM bronze"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "73988f7d-a4d9-4f3a-ba2f-51e89554314e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the following cell to delete the tables and files associated with this lesson."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "96518f5a-0e5f-4a4c-8f8b-55a12b2b315a",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "DA.cleanup()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "f2481f3d-4291-46fb-8758-bb06c5f74e4d",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "notebookName": "ADE 2.1 - Auto Load to Multiplex Bronze",
   "dashboards": [],
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "language": "python",
   "widgets": {},
   "notebookOrigID": 4185566132529457
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}