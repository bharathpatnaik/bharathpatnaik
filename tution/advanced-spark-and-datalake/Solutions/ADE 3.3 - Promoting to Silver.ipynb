{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Promoting to Silver\n",
    "\n",
    "Here we'll pull together the concepts of streaming from Delta Tables, deduplication, and quality enforcement to finalize our approach to our silver table.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/ade/ADE_arch_heartrate_silver.png\" width=\"60%\" />\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lesson, students will be able to:\n",
    "- Apply table constraints to Delta Lake tables\n",
    "- Use flagging to identify records failing to meet certain conditions\n",
    "- Apply de-duplication within an incremental microbatch\n",
    "- Use **`MERGE`** to avoid inserting duplicate records to a Delta Lake table"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "31e0d388-6ca5-422b-ab19-9826a77b8004",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "7a8be236-de00-43da-94ac-732f391d3953",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%run ../Includes/Classroom-Setup-4.3"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "a2ef7ad0-8bbc-471b-b9d7-244cc2c6b81b",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Start by creating our **`heart_rate_silver`** table"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "b923cded-12b9-43dc-8b05-d2582d349af2",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS heart_rate_silver\n",
    "  (device_id LONG, time TIMESTAMP, heartrate DOUBLE, bpm_check STRING)\n",
    "USING DELTA\n",
    "LOCATION '${da.paths.user_db}/heart_rate_silver'"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "1c175f50-5011-4220-a8b0-1fccae28888a",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Table Constraint\n",
    "Add a table constraint before inserting data. Name this constraint **`dateWithinRange`** and make sure that the time is greater than January 1, 2017."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "b4eaca52-f295-483f-b3bb-38ba6aac5067",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "-- ANSWER\n",
    "ALTER TABLE heart_rate_silver ADD CONSTRAINT dateWithinRange CHECK (time > '2017-01-01');"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "5b16199e-df49-4df1-a1c6-9d389b8b2d0b",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that adding and removing constraints is recorded in the transaction log."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "11feb628-ef11-4ef9-be09-135f5102164c",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY heart_rate_silver"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "c90cf1e3-b0d4-4111-a148-8cad8b203904",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define a Streaming Read and Transformation\n",
    "Using the cell below we will create a streaming read that includes:\n",
    "1. A filter for the topic **`bpm`**\n",
    "2. Logic to flatten the JSON payload and cast data to the appropriate schema\n",
    "3. A **`bpm_check`** column to flag negative records\n",
    "4. A duplicate check on **`device_id`** and **`time`** with a 30 second watermark on **`time`**"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "bc7e774c-7b36-4b31-831e-e8d9cbe5d084",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "json_schema = \"device_id LONG, time TIMESTAMP, heartrate DOUBLE\"\n",
    "\n",
    "streaming_df = (spark.readStream\n",
    "                     .table(\"bronze\")\n",
    "                     .filter(\"topic = 'bpm'\")\n",
    "                     .select(F.from_json(F.col(\"value\").cast(\"string\"), json_schema).alias(\"v\"))\n",
    "                     .select(\"v.*\", F.when(F.col(\"v.heartrate\") <= 0, \"Negative BPM\")\n",
    "                                     .otherwise(\"OK\")\n",
    "                                     .alias(\"bpm_check\"))\n",
    "                     .withWatermark(\"time\", \"30 seconds\")\n",
    "                     .dropDuplicates([\"device_id\", \"time\"]))"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "ea7a5caf-ee1a-4806-ab26-4d832f5c57b5",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Upsert Query\n",
    "Below, the upsert class used in the previous notebooks is provided."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "778fb936-7d46-4c3b-b7d3-8f7282bdf999",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Upsert:\n",
    "    def __init__(self, sql_query, update_temp=\"stream_updates\"):\n",
    "        self.sql_query = sql_query\n",
    "        self.update_temp = update_temp \n",
    "        \n",
    "    def upsert_to_delta(self, micro_batch_df, batch):\n",
    "        micro_batch_df.createOrReplaceTempView(self.update_temp)\n",
    "        micro_batch_df._jdf.sparkSession().sql(self.sql_query)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "d9334159-13c1-4b24-ae6e-1583b482768b",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the cell below to define the upsert query to instantiate our class. \n",
    "\n",
    "Alternatetively, <a href=\"https://docs.databricks.com/delta/delta-update.html#upsert-into-a-table-using-merge&language-python\" target=\"_blank\">consult the documentation</a> and try implementing this using the **`DeltaTable`** Python class."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "9d7a3088-9fc3-494a-a228-4747f8b5fc35",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ANSWER\n",
    "sql_query = \"\"\"\n",
    "  MERGE INTO heart_rate_silver a\n",
    "  USING stream_updates b\n",
    "  ON a.device_id=b.device_id AND a.time=b.time\n",
    "  WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\"\n",
    "\n",
    "streaming_merge = Upsert(sql_query)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "906fde9d-e30e-4e75-802f-d956b48955ab",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "# ANSWER\n",
    "from delta.tables import *\n",
    "\n",
    "delta_table = DeltaTable.forName(spark, \"heart_rate_silver\")\n",
    "\n",
    "def upsert_to_delta(micro_batch_df, batch):\n",
    "    (delta_table.alias(\"a\")\n",
    "                .merge(micro_batch_df.alias(\"b\"), \"a.device_id=b.device_id\")\n",
    "                .whenNotMatchedInsertAll()\n",
    "                .execute())"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "1aa9a42f-10f1-4bb4-b6bc-6716f023043e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Apply Upsert and Write\n",
    "Now execute a write with trigger-available-now logic to process all existing data from the bronze table."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "3cf901af-49e0-400a-a5e7-18e2252ab4b6",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def process_silver_heartrate():\n",
    "    query = (streaming_df.writeStream\n",
    "                         .foreachBatch(streaming_merge.upsert_to_delta)\n",
    "                         .outputMode(\"update\")\n",
    "                         .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/recordings\")\n",
    "                         .trigger(availableNow=True)\n",
    "                         .start())\n",
    "    query.awaitTermination()\n",
    "    \n",
    "process_silver_heartrate()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "bb8b4e1d-f310-4a1a-9d08-426629405885",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "We should see the same number of total records in our silver table as the deduplicated count from the lesson 2.5, and a small percentage of these will correctly be flagged with \"Negative BPM\"."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "1172ee80-5c03-4402-86fa-9b1f5711b3ab",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "new_total = spark.read.table(\"heart_rate_silver\").count()\n",
    "\n",
    "print(f\"Lesson #5: {731987:,}\")\n",
    "print(f\"New Total: {new_total:,}\")"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "5f9d3c25-cc4e-4600-9db3-6bad9b2194ed",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT COUNT(*)\n",
    "FROM heart_rate_silver\n",
    "WHERE bpm_check = \"Negative BPM\""
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "6442ebfc-e8a9-4209-9f2a-a1d46478d6dd",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now land a new batch of data and propagate changes through bronze into the silver table.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_32.png\"> The following two methods were recreated for us from previous lessons"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "7d47b19a-90ff-478d-b703-61eb8336558d",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "DA.daily_stream.load() # Load a day's worth of data\n",
    "DA.process_bronze()    # Execute 1 iteration of the daily to bronze stream\n",
    "\n",
    "process_silver_heartrate()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "7b7bae55-5296-4c82-8e76-ddb4038732b5",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "end_total = spark.read.table(\"heart_rate_silver\").count()\n",
    "\n",
    "print(f\"Lesson #5:   {731987:,}\")\n",
    "print(f\"New Total:   {new_total:,}\")\n",
    "print(f\"End Total: {end_total:,}\")"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "e5e91de8-d1ac-4c79-a857-f17cb6ba18a2",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the following cell to delete the tables and files associated with this lesson."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "3fc8843b-3280-4f07-88f5-fd9137d3c912",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "DA.cleanup()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "48f1ecba-0d05-4062-a354-71748a318be9",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "notebookName": "ADE 3.3 - Promoting to Silver",
   "dashboards": [],
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "language": "python",
   "widgets": {},
   "notebookOrigID": 4185566132529091
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}