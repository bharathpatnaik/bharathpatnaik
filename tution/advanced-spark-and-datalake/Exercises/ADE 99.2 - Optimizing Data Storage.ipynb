{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Optimizing Data Storage with Delta Lake\n",
    "\n",
    "Databricks supports a number of optimizations for clustering data and improving directory and file skipping while scanning and loading data files. While some of these optimizations will use the word \"index\" in describing the process used, these indices differ from the algorithms many users will be familiar with from traditional SQL database systems.\n",
    "\n",
    "In this notebook we'll explore how optional data storage and optimization settings on Delta Lake interact with file size and data skipping.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lessons, students will be able to:\n",
    "- Describe default behavior for statistics collection and file skipping on Delta Lake\n",
    "- Identify columns well-suited to partitioning\n",
    "- Use **`OPTIMIZE`** to compact small files\n",
    "- Apply Z-order to optimize file skipping on high cardinality fields\n",
    "- Use Bloom filters to speed up queries on fields with arbitrary text"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "540945a6-1bee-4598-ae49-2c2a6fe2f4c5",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%run ../Includes/Classroom-Setup-1.2"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "55cc42ca-239b-4a67-9863-5a740126628f",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a Delta Table\n",
    "\n",
    "The following CTAS statement creates a simple, unpartitioned external Delta Lake table from a sample dataset"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "40fa4b9f-622d-4d12-a425-c0aaddb99a1f",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE no_part_table\n",
    "LOCATION \"${da.paths.working_dir}/no_part_table\"\n",
    "AS SELECT * FROM raw_data"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "9b5abec1-8f71-44a4-9d0c-d3d71b32c3ed",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Schema Considerations\n",
    "When configuring tables in Delta Lake, make sure you consider the following.\n",
    "\n",
    "### Precision\n",
    "Both numeric and datetime types should be stored with the correct precision specified to:\n",
    "1. Ensure integrity with source systems\n",
    "1. Maintain precision and avoid rounding errors for downstream queries\n",
    "1. Avoid unnecessary storage costs (note the significant differences in bytes for <a href=\"https://spark.apache.org/docs/latest/sql-ref-datatypes.html\" target=\"_blank\">numeric types</a>)\n",
    "\n",
    "### Datetime Filtering\n",
    "If data will be frequently filtered by year, year & month, day of week, date, or another datetime value, consider calculating these values at write time if not present in original data. (Pushdown filters work best on fields present in a table).\n",
    "\n",
    "### Case Sensitivity\n",
    "Spark does not differentiate case by default.\n",
    "\n",
    "### Un-Nest Important Fields for Filtering\n",
    "Extract fields that might be useful for indexing or filtering to increase performance.\n",
    "\n",
    "### Place Important Fields Early in the Schema\n",
    "Fields that will be used for filtering and optimizations should appear at the beginning of the schema declaration."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "4f20d323-2b58-4d01-b245-4dbefa2241b3",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## File Skipping with Delta Lake File Statistics\n",
    "\n",
    "By default, Delta Lake will capture statistics on the first 32 columns that appear in a table. These statistics indicate:\n",
    "- the total number of records per file\n",
    "- minimum value in each column \n",
    "- maximum value in each column\n",
    "- null value counts for each of the columns\n",
    "\n",
    "**NOTE**: These statistics are generally uninformative for string fields with very high cardinality (such as free text fields). You can omit these fields from statistic collection by <a href=\"https://docs.databricks.com/delta/optimizations/file-mgmt.html#data-skipping\" target=\"_blank\">moving them outside the first 32 columns or changing the number of columns on which statistics are collected</a>.  Nested fields count when determining the first 32 columns, for example 4 struct fields with 8 nested fields will total to the 32 columns."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "3127f1fb-3120-422a-bb17-f9aa6408d338",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reviewing Statistics in the Transaction Log\n",
    "\n",
    "Statistics are recorded in the Delta Lake transaction log files. Files are initially committed in the JSON format, but are compacted to Parquet format automatically to accelerate metadata retrieval.\n",
    "\n",
    "Transaction logs can be viewed in the **`_delta_log`** directory within the table location."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "daaafd00-7d35-4e1d-8ffc-47a88483dba8",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "files = dbutils.fs.ls(f\"{DA.paths.working_dir}/no_part_table/_delta_log\")\n",
    "display(files)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "a81a1016-e359-44ba-bda9-0bac83c8e7d1",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "JSON log files can be easily parsed with Spark. Statistics for each file are accessible in the **`add`** column.\n",
    "\n",
    "When a query with a selective filter (**`WHERE`** clause) is executed against a Delta Lake table, the query optimizer uses the information stored in the transaction logs to identify files that **may** contain records matching the conditional filter."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "4a568e81-21d1-4869-82bf-fdfcfd3acc43",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "display(spark.read.json(f\"{DA.paths.working_dir}/no_part_table/_delta_log/00000000000000000000.json\"))"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "e6be55c4-3ab8-47ff-b03b-ef7643f9f21e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that columns used for Z-ordering need to have statistics collected. Even without additional optimization metrics, statistics will always be leveraged for file skipping.\n",
    "\n",
    "**NOTE**: Calculating statistics on free-form text fields (product reviews, user messages, etc.) can be time consuming. For best performance, set these fields later in the schema and <a href=\"https://docs.databricks.com/delta/optimizations/file-mgmt.html#data-skipping\" target=\"_blank\">change the number of columns that statistics are collected on</a>."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "2658a0ab-13dc-40c8-8049-eadd93e59c39",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Partitioning Delta Lake Tables\n",
    "\n",
    "The partitioning method used in Delta Lake is similar to that used by Hive or Spark with Parquet (recall that Delta Lake data files are stored as Parquet).\n",
    "\n",
    "When a column is used to partition a table, each unique value found in that column will create a separate directory for data. When choosing partition columns, it's good to consider the following:\n",
    "1. How will the table be used?\n",
    "   - **Partitioning can help optimize performance for operational OR analytic queries (rarely both)**\n",
    "1. How many total values will be present in a column?\n",
    "   - **Low cardinality fields should be used for partitioning**\n",
    "1. How many total records will share a given value for a column?\n",
    "   - **Partitions should be at least 1 GB in size (or larger depending on total table size)**\n",
    "1. Will records with a given value continue to arrive indefinitely?\n",
    "   - **Discrete datetime values can allow partitions to be optimized and archived once late-arriving data is processed**\n",
    "\n",
    "**NOTE**: When in doubt, do not partition data at all. Other data skipping features in Delta Lake can achieve similar speeds as partitioning, but data that is over-partitioned or incorrectly partitioned will suffer greatly (and require a full rewrite of all data files to remedy).\n",
    "\n",
    "Columns representing measures of time and low-cardinality fields used frequently in queries are good candidates for partitioning. The code below creates a table partitioned by date using <a href=\"https://docs.databricks.com/delta/delta-batch.html#deltausegeneratedcolumns\" target=\"_blank\">generated columns</a>. Generated columns will be stored the same way other columns are, but will be calculated at write time using the logic provided when the table was defined."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "697f217d-94a9-4b89-b345-1a6b3e4f5a59",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE date_part_table (\n",
    "  key STRING,\n",
    "  value BINARY,\n",
    "  topic STRING,\n",
    "  partition LONG,\n",
    "  offset LONG,\n",
    "  timestamp LONG,\n",
    "  p_date DATE GENERATED ALWAYS AS (CAST(CAST(timestamp/1000 AS timestamp) AS DATE))\n",
    ")\n",
    "PARTITIONED BY (p_date)\n",
    "LOCATION '${da.paths.working_dir}/date_part_table'"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "53cdca72-48b9-49f7-a2d1-78cba19bd531",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "(spark.table(\"raw_data\")\n",
    "      .write.mode(\"append\")\n",
    "      .saveAsTable(\"date_part_table\"))"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "9238fd11-4539-40b0-8822-4e1ff2f1c24b",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Listing the location used for the table reveals that the unique values in the partition column are used to generate data directories. Note that the Parquet format used to store the data for Delta Lake leverages these partitions directly when determining column value (the column values for **`p_date`** are not stored redundantly within the data files)."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "5c41add9-2fec-4d41-9127-b8bb3c8732d4",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "files = dbutils.fs.ls(f\"{DA.paths.working_dir}/date_part_table\")\n",
    "display(files)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "49f47801-379c-4bd8-92f6-c97bd4f0b79e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data in this table look largely the same, except that more files were written because of the separation of data into separate directories based on the date."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "1a5cfb04-421c-45bf-9bc3-93769a140e0d",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "path = f\"{DA.paths.working_dir}/date_part_table/_delta_log/00000000000000000001.json\"\n",
    "df = spark.read.json(path)\n",
    "display(df)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "dedcc749-1516-4909-af14-20fa7411dbf6",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "When running a query that filters data on a column used for partitioning, partitions not matching a conditional statement will be skipped entirely. Delta Lake also have several operations (including **`OPTIMIZE`** commands) that can be applied at the partition level.\n",
    "\n",
    "Note that because data files will be separated into different directories based on partition values, files cannot be combined or compacted across these partition boundaries. Depending on the size of data in a given table, the \"right size\" for a partition will vary, but if most partitions in a table will not contain at least 1GB of data, the table is likely over-partitioned, which will lead to slowdowns for most general queries."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "927af5d4-baaf-4f3b-b0f4-bac13dbb01ef",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT p_date, COUNT(*) \n",
    "FROM date_part_table \n",
    "GROUP BY p_date"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "3fc91d8f-9a7a-40b9-a0e0-76051d1c6440",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Computing Stats\n",
    "\n",
    "Users can <a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/sql-ref-syntax-aux-analyze-table.html\" target=\"_blank\">manually specify relational entities for which statistics should be calculated with **`ANALYZE`**</a>. While analyzing a table or a subset of columns for a table is not equivalent to indexing, it can allow the query optimizer to select more efficient plans for operations such as joins.\n",
    "\n",
    "Statistics can be collected for all tables in a database, a specific table, a partition of a table, or a subset of columns in a table.\n",
    "\n",
    "Below, statistics are computed for the **`timestamp`** column."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "b88a27ea-0756-4c0d-82d7-075c4eecbda4",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "ANALYZE TABLE no_part_table \n",
    "COMPUTE STATISTICS FOR COLUMNS timestamp"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "43e2c05e-9977-44f4-81a5-93bdb17fc9a9",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "These statistics can be seen by running **`DESCRIBE EXTENDED`** on the table and column."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "b840cc51-e907-47c0-bad8-88066e7ad539",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "DESCRIBE EXTENDED no_part_table timestamp"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "408815bb-0a4d-46d1-9bd0-98b61c85a482",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## File Compaction\n",
    "Delta Lake supports the **`OPTIMIZE`** operation, which performs file compaction. The <a href=\"https://docs.databricks.com/delta/optimizations/file-mgmt.html#autotune-based-on-table-size\" target=\"_blank\">target file size can be auto-tuned</a> by Databricks, and is typically between 256 MB and 1 GB depending on overall table size.\n",
    "\n",
    "Note that data files cannot be combined across partitions. As such, some tables will benefit from not using partitions to minimize storage costs and total number of files to scan.\n",
    "\n",
    "**NOTE**: Optimization schedules will vary depending on the nature of the data and how it will be used downstream. Optimization can be scheduled for off-hours to reduce competition for resources with important workloads. Delta Live Tables has added functionality to automatically optimize tables."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "2aed64a3-90f0-43cd-8187-08d8d56df664",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Z-Ordering\n",
    "\n",
    "Z-ordering is a technique to collocate related information in the same set of files. This co-locality is automatically used by Databricks data-skipping algorithms to dramatically reduce the amount of data that needs to be read.\n",
    "\n",
    "Don't worry about <a href=\"https://en.wikipedia.org/wiki/Z-order_curve\" target=\"_blank\">the math</a> (tl;dr: Z-order maps multidimensional data to one dimension while preserving locality of the data points).\n",
    "\n",
    "Multiple columns can be used for Z-ordering, but the algorithm loses some efficiency with each additional column. The best columns for Z-ordering are high cardinality columns that will be used commonly in queries.\n",
    "\n",
    "Z-ordering must be executed at the same time as **`OPTIMIZE`**, as it requires rewriting data files.\n",
    "\n",
    "Below is the code to Z-order and optimize the **`date_part_table`** by **`timestamp`** (this might be useful for regular queries within granular time ranges)."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "f007d602-3ed9-4f36-96d4-f222b3a6808c",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "OPTIMIZE date_part_table\n",
    "ZORDER BY (timestamp)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "0521b766-dd7a-4270-b921-89408bd8b457",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the metrics will provide an overview of what happened during the operation; reviewing the table history will also provide this information."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "22a4f8c0-d759-43c5-870b-6fedfec65c7f",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY date_part_table"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "605765f9-653d-4467-b2b3-1f303fc1924e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bloom Filter Indexes\n",
    "\n",
    "While Z-order provides useful data clustering for high cardinality data, it's often most effective when working with queries that filter against continuous numeric variables.\n",
    "\n",
    "Bloom filters provide an efficient algorithm for probabilistically identifying files that may contain data using fields containing arbitrary text. Appropriate fields would include hashed values, alphanumeric codes, or free-form text fields.\n",
    "\n",
    "Bloom filters calculate indexes that indicate the likelihood a given value **could** be in a file; the size of the calculated index will vary based on the number of unique values present in the field being indexed and the configured tolerance for false positives.\n",
    "\n",
    "**NOTE**: A false positive would be a file that the index thinks could have a matching record but does not. Files containing data matching a selective filter will never be skipped; false positives just mean that extra time was spent scanning files without matching records.\n",
    "\n",
    "Looking at the distribution for the **`key`** field, this is an ideal candidate for this technique."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "2a996d45-e45c-4cd9-87eb-593bb190de61",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT key, count(*) FROM no_part_table GROUP BY key ORDER BY count(*) ASC"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "11fdb4ff-aa36-4168-9c32-973b05266847",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "The code below sets a Bloom filter index on the **`key`** field with a false positivity allowance of 0.1%."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "1a44cb1b-4774-42b6-90fa-20d004e77376",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "CREATE BLOOMFILTER INDEX\n",
    "ON TABLE date_part_table\n",
    "FOR COLUMNS(key OPTIONS (fpp=0.1, numItems=200))"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "4928a7ba-ab16-406a-a080-8eeec7765d1b",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hands-On\n",
    "\n",
    "Go through the process of Z-ordering and adding a Bloom filter index to the **`no_part_table`**. Review the history for the table to confirm the operations were successful."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "5ad2c860-9080-4461-b33b-990906380a7d",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql \n",
    "-- TODO\n",
    "<FILL-IN>"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "1739b718-64a4-4de8-830e-45aa1443743f",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: Adding a bloom filter will not create the filter for existing file-parts.  Only newly written files will have a filter created.  Optimizing an unoptimized delta table typically will result in writing all new files and therefore populate the filter.  But if the table is already optimized this will not work and you may need to copy the table instead."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "c42f2384-9dfe-44f9-9c7f-e222d6ead707",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the following cell to delete the tables and files associated with this lesson."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "92c7e03a-ce4f-4e4b-8aa2-7f65f41ab21e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "DA.cleanup()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "38b4e39e-acb5-4d78-aa02-bb68c51a46cf",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "notebookName": "ADE 99.2 - Optimizing Data Storage",
   "dashboards": [],
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "language": "python",
   "widgets": {},
   "notebookOrigID": 4185566132530110
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}