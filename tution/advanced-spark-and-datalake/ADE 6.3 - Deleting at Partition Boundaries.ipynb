{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deleting at Partition Boundaries\n",
    "\n",
    "While we've deleted our PII from our silver tables, we haven't dealt with the fact that this data still exists in our **`bronze`** table.\n",
    "\n",
    "Note that because of stream composability and the design choice to use a multiplex bronze pattern, enabling Delta Change Data Feed (CDF) to propagate delete information would require redesigning each of our pipelines to take advantage of this output. Without using CDF, modification of data in a table will break downstream composability.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/ade/ADE_arch_bronze.png\" width=\"60%\" />\n",
    "\n",
    "In this notebook, you'll learn how to delete partitions of data from Delta Tables and how to configure incremental reads to allow for these deletes.\n",
    "\n",
    "This functionality is not only useful for permanently deleting PII, but this same pattern can be applied in companies that just want to expunge data older than a certain age from a given table. Similarly, data could be backed up to a cheaper storage tier, and then safely deleted from \"active\" or \"hot\" Delta tables to drive savings on cloud storage.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, students will be able to:\n",
    "- Delete data using partition boundaries\n",
    "- Configure downstream incremental reads to safely ignore these deletions\n",
    "- Use **`VACUUM`** to review files to be deleted and commit deletes\n",
    "- Union archived data with production tables to recreate a full historic dataset"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "1bcefa42-c23a-4ae5-9ddd-14466c053197",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Begin by running our setup script."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "2b19b7a9-31bc-41d3-83a2-69c78881fec3",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%run ../Includes/Classroom-Setup-7.3"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "cbff3cee-b297-4ca6-a44a-7cbfa051b6cf",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our Delta table is partitioned by two fields. \n",
    "\n",
    "Our top level partition is the **`topic`** column. \n",
    "\n",
    "Run the cell to note the 3 partition directories (and the Delta Log directory) that collectively comprise our **`bronze`** table."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "660f4ed1-313d-4ef9-abd5-c59e9724a018",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "files = dbutils.fs.ls(f\"{DA.paths.user_db}/bronze\")\n",
    "display(files)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "1b3787c6-91ef-41ec-afb4-7de30ce4090f",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our 2nd level partition was on our **`week_part`** column, which we derived as the year and week of year. There are around 20 directories currently present at this level."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "2fe96165-7d3c-431c-a8cf-156c55a2d4fc",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "files = dbutils.fs.ls(f\"{DA.paths.user_db}/bronze/topic=user_info\")\n",
    "display(files)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "fbad8af8-2170-40c9-9ed2-720f6d6b6571",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that in our current dataset, we're tracking only a small number of total users in these files."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "f60ab54d-ee35-4fb9-9116-07df9a1c3760",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "total = (spark.table(\"bronze\")\n",
    "              .filter(\"topic='user_info'\")\n",
    "              .filter(\"week_part<='2019-48'\")\n",
    "              .count())\n",
    "         \n",
    "print(f\"Total: {total}\")"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "62d67b45-3e0f-4d33-bd6f-1e2c135b5a18",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Archiving Data\n",
    "If a company wishes to maintain an archive of historic records (but only maintain recent records in production tables), cloud-native settings for auto-archiving data can be configured to move data files automatically to lower-cost storage locations.\n",
    "\n",
    "The cell below simulates this process (here using copy instead of move). \n",
    "\n",
    "Note that because only the data files and partition directories are being relocated, the resultant table will be Parquet by default.\n",
    "\n",
    "**NOTE**: For best performance, directories should have **`OPTIMIZE`** run to condense small files. Because valid and stale data files are stored side-by-side in Delta Lake files, partitions should also have **`VACUUM`** executed prior to moving any Delta Lake data to a pure Parquet table to ensure only valid files are copied."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "b3daa7ca-f682-4c2e-8a16-fc50e5d0f78c",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "archive_path = f\"{DA.paths.working_dir}/pii_archive\"\n",
    "source_path = f\"{DA.paths.user_db}/bronze/topic=user_info\"\n",
    "\n",
    "files = dbutils.fs.ls(source_path)\n",
    "[dbutils.fs.cp(f[0], f\"{archive_path}/{f[1]}\", True) for f in files if f[1][-8:-1] <= '2019-48'];\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS user_info_archived\n",
    "USING parquet\n",
    "LOCATION '{archive_path}'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"MSCK REPAIR TABLE user_info_archived\")\n",
    "\n",
    "display(spark.sql(\"SELECT COUNT(*) FROM user_info_archived\"))"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "dde582b0-4b00-4c6c-808f-320280fda333",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the directory structure was maintained as files were copied."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "485d5952-ecba-4bc8-abba-a00e1868ba1c",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "files = dbutils.fs.ls(archive_path)\n",
    "display(files)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "5be83648-7cbf-4680-b64a-7d0dd6bcc421",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deleting at a Partition Boundary\n",
    "Here we'll model deleting all **`user_info`** that was received before week 49 of 2019.\n",
    "\n",
    "Note that we are deleting cleanly along partition boundaries. All the data contained in the specified **`week_part`** directories will be removed from our table."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "a22bef7a-f6d4-42a6-85a7-099795905291",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "DELETE FROM bronze \n",
    "WHERE topic = 'user_info'\n",
    "AND week_part <= '2019-48'"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "8ad9ae46-b6cd-4443-8c64-972fa55955eb",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can confirm this delete processed successfully by looking at the history. The **`operationMetrics`** column will indicate the number of removed files."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "7dfbdd73-08b8-4dcc-8617-c1bf3613e5b2",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY bronze"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "ac6d703b-8438-4396-b73f-b5696a5b403c",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "When deleting along partition boundaries, we don't write out new data files; recording the files as removed in the Delta log is sufficient. \n",
    "\n",
    "However, file deletion will not actually occur until we **`VACUUM`** our table. \n",
    "\n",
    "Note that all of our week partitions still exist in our **`user_info`** directory and that data files still exist in each week directory."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "c6d35f04-a07d-49d4-9f46-43491a51c455",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "files = dbutils.fs.ls(f\"{source_path}/week_part=2019-48\")\n",
    "display(files)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "fecf727f-4072-4b64-b500-ccbc93a2b979",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reviewing and Committing Deletes\n",
    "\n",
    "By default, the Delta engine will prevent **`VACUUM`** operations with less than 7 days of retention. The cell below overrides this check."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "bf07b8b2-7853-4767-8437-50145cb79b8b",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", False)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "4c8c55f2-4395-455c-bf64-88f832564f9e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adding the **`DRY RUN`** keyword to the end of our **`VACUUM`** statement allows us to preview files to be deleted before they are permanently removed. \n",
    "\n",
    "Note that at this moment we could still recover our deleted data by running:\n",
    "\n",
    "<strong><code>\n",
    "RESTORE bronze<br/>\n",
    "TO VERSION AS OF {version}\n",
    "</code></strong>"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "e7eaeb70-eba0-4a98-ae88-a0f89714dc41",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "VACUUM bronze RETAIN 0 HOURS DRY RUN"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "4d76011c-beec-4eea-9c37-3bcc6606b70d",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Executing the **`VACUUM`** command below permanently deletes these files."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "4914607c-cfc2-4eb6-9ba4-4f120f5b585b",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "VACUUM bronze RETAIN 0 HOURS"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "8c7f1177-473d-4053-bc7a-6c71cb7b9087",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "For safety, it's best to always re-enable our **`retentionDurationCheck`**. In production, you should avoid overriding this check whenever possible (if other operations are acting against files not yet committed to a Delta table and written before the retention threshold, **`VACUUM`** can result in data corruption)."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "5569cbe2-2af5-4782-8501-6cea3f739cad",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", True)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "60cc8e1e-06dc-44de-be24-f8b51a2f82d8",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that empty directories will eventually be cleaned up with **`VACUUM`**, but may not always be deleted as they are emptied of data files."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "2f404b01-27ea-4345-a01e-98774f28b0dc",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "try:\n",
    "    files = dbutils.fs.ls(source_path)\n",
    "    print(\"The files NOT YET deleted.\")\n",
    "except Exception as e:\n",
    "    print(\"The files WERE deleted.\")"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "9eb9dc72-5b08-4abe-8de3-d29dedc34ca1",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "As such, querying the **`bronze`** table with the same filters used in our delete statement should yield 0 records."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "6ae152e4-0fe6-4754-8e4f-1b82f70069fc",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM bronze \n",
    "WHERE topic='user_info' AND \n",
    "      week_part <= '2019-48'"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "32f5e06c-cfb6-4ef1-a186-8bb418e7f227",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Recreating Full Table History\n",
    "\n",
    "Note that because Parquet using directory partitions as columns in the resulting dataset, the data that was backed up no longer has a **`topic`** field in its schema.\n",
    "\n",
    "The logic below addresses this while calling **`UNION`** on the archived and production datasets to recreate the full history of the **`user_info`** topic."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "30aeabdd-89a4-47e9-85b8-4d3664b0b70b",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "WITH full_bronze_user_info AS (\n",
    "\n",
    "  SELECT key, value, partition, offset, timestamp, date, week_part \n",
    "  FROM bronze \n",
    "  WHERE topic='user_info'\n",
    "  \n",
    "  UNION SELECT * FROM user_info_archived) \n",
    "  \n",
    "SELECT COUNT(*) FROM full_bronze_user_info"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "d3e9aea7-562a-4583-8af9-de394ca8b502",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Updating Streaming Reads to Ignore Changes\n",
    "\n",
    "The cell below condenses all the code used to perform streaming updates to our **`users`** table.\n",
    "\n",
    "If you try to execute this code right now, you'll raise an exception\n",
    "> Detected deleted data from streaming source\n",
    "\n",
    "Line 22 of the cell below adds the **`.option(\"ignoreDeletes\", True)`** to the DataStreamReader. This option is all that is necessary to enable streaming processing from Delta tables with partition deletes."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "5c45001c-2781-4b54-be29-1deb7b3d63fb",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "schema = \"\"\"\n",
    "    user_id LONG, \n",
    "    update_type STRING, \n",
    "    timestamp FLOAT, \n",
    "    dob STRING, \n",
    "    sex STRING, \n",
    "    gender STRING, \n",
    "    first_name STRING, \n",
    "    last_name STRING, \n",
    "    address STRUCT<\n",
    "        street_address: STRING, \n",
    "        city: STRING, \n",
    "        state: STRING, \n",
    "        zip: INT\n",
    "    >\"\"\"\n",
    "\n",
    "salt = \"BEANS\"\n",
    "\n",
    "unpacked_df = (spark.readStream\n",
    "                    .option(\"ignoreDeletes\", True)     # This is new!\n",
    "                    .table(\"bronze\")\n",
    "                    .filter(\"topic = 'user_info'\")\n",
    "                    .dropDuplicates()\n",
    "                    .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\")).select(\"v.*\")\n",
    "                    .select(F.sha2(F.concat(F.col(\"user_id\"), F.lit(salt)), 256).alias(\"alt_id\"),\n",
    "                            F.col('timestamp').cast(\"timestamp\").alias(\"updated\"),\n",
    "                            F.to_date('dob','MM/dd/yyyy').alias('dob'),\n",
    "                            'sex', 'gender','first_name','last_name','address.*', \"update_type\"))\n",
    "\n",
    "\n",
    "\n",
    "def batch_rank_upsert(microBatchDF, batchId):\n",
    "    from pyspark.sql.window import Window\n",
    "    \n",
    "    window = Window.partitionBy(\"alt_id\").orderBy(F.col(\"updated\").desc())\n",
    "    \n",
    "    (microBatchDF\n",
    "        .filter(F.col(\"update_type\").isin([\"new\", \"update\"]))\n",
    "        .withColumn(\"rank\", F.rank().over(window)).filter(\"rank == 1\").drop(\"rank\")\n",
    "        .createOrReplaceTempView(\"ranked_updates\"))\n",
    "    \n",
    "    microBatchDF._jdf.sparkSession().sql(\"\"\"\n",
    "        MERGE INTO users u\n",
    "        USING ranked_updates r\n",
    "        ON u.alt_id=r.alt_id\n",
    "        WHEN MATCHED AND u.updated < r.updated\n",
    "          THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED\n",
    "          THEN INSERT *\n",
    "    \"\"\")"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "4559a610-97a7-4be5-b485-13fc54d026a3",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "query = (unpacked_df.writeStream\n",
    "                    .foreachBatch(batch_rank_upsert)\n",
    "                    .outputMode(\"update\")\n",
    "                    .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/batch_rank_upsert\")\n",
    "                    .trigger(once=True)\n",
    "                    .start())    \n",
    "\n",
    "query.awaitTermination()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "27fe8d36-9c06-40e1-b39a-9b2db7a0e7a2",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that we may see the table version increment as this code completes."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "6574dcb6-c994-43bd-974f-65b412d75153",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY users"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "b550a3ec-e2a5-4801-90bf-40292167f921",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, by examining the Delta log file this version, we'll note that the file written out is just indicating the data change, but that no new records were added or modified."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "374ecc79-5a4a-472f-9820-a4eb20b0e933",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "users_log_path = f\"{DA.paths.user_db}/users/_delta_log\"\n",
    "files = dbutils.fs.ls(users_log_path)\n",
    "\n",
    "max_version = max([file.name for file in files if file.name.endswith(\".json\")])\n",
    "display(spark.read.json(f\"{users_log_path}/{max_version}\"))"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "bb0ef8d0-c85e-470e-8bac-f5827c2eb442",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Next Steps\n",
    "While we did not modify data in our **`workout`** or **`bpm`** partitions, because these read from the same **`bronze`** table we'll need to also update their DataStreamReader logic to ignore changes."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "f4a46336-9c5c-409d-9738-674f569229ef",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the following cell to delete the tables and files associated with this lesson."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "e26c9f50-6dca-40a4-8984-4a7699dcb9ad",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "DA.cleanup()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "45db2ece-119e-4586-857a-bb51617bd527",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "notebookName": "ADE 6.3 - Deleting at Partition Boundaries",
   "dashboards": [],
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "language": "python",
   "widgets": {},
   "notebookOrigID": 4185566132530004
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}