{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Streaming Design Patterns\n",
    "\n",
    "The Lakehouse has been designed from the beginning to work seamlessly with datasets that grow infinitely over time. While Spark Structured Streaming is often positioned as a near real-time data processing solution, it combines with Delta Lake to also provide easy batch processing of incremental data while drastically simplifying the overhead required to track data changes over time.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lessons, student will be able to:\n",
    "- Use Structured Streaming to complete simple incremental ETL\n",
    "- Perform incremental writes to multiple tables\n",
    "- Incrementally update values in a key value store\n",
    "- Process Change Data Capture (CDC) data into Delta Tables using **`MERGE`**\n",
    "- Join two incremental tables\n",
    "- Join incremental and batch tables"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "5a9ce081-fd3c-4fe0-a0c9-08a5823c5ca1",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the following script to setup necessary variables and clear out past runs of this notebook."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "29e7db8d-eb64-490f-80e1-055bcae74837",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%run ../Includes/Classroom-Setup-2.1"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "404cab76-c070-4d62-a94f-6858e61e1456",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simple Incremental ETL\n",
    "\n",
    "Likely the highest volume of data being processed by most organizations could largely be describing as moving data from one location to another while applying light transformations and validations. \n",
    "\n",
    "As most source data continues to grow as time passes, it's appropriate to refer to this data as incremental (sometimes also referred to as streaming data). \n",
    "\n",
    "Structured Streaming and Delta Lake make incremental ETL easy. \n",
    "\n",
    "Below we'll create a simple table and insert some values."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "e04e1aaf-05c6-497a-94fc-1182b5441155",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "\n",
    "CREATE TABLE bronze \n",
    "(id INT, name STRING, value DOUBLE); \n",
    "\n",
    "INSERT INTO bronze\n",
    "VALUES (1, \"Yve\", 1.0),\n",
    "       (2, \"Omar\", 2.5),\n",
    "       (3, \"Elia\", 3.3)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "084c7fc5-2a9e-44ea-b502-ba5ff71b7461",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell defines an incremental read on the table just created using Structured Streaming, adds a field to capture when the record was processed, and writes out to a new table as a single batch."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "d4af675c-e418-46b6-892e-f1906a818196",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def update_silver():\n",
    "    query = (spark.readStream\n",
    "                  .table(\"bronze\")\n",
    "                  .withColumn(\"processed_time\", F.current_timestamp())\n",
    "                  .writeStream.option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/silver\")\n",
    "                  .trigger(availableNow=True)\n",
    "                  .table(\"silver\"))\n",
    "    \n",
    "    query.awaitTermination()\n"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "310fc30d-e351-4ef2-8ceb-203326a926f5",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "While this code uses Structured Streaming, it's appropriate to think of this as a triggered batch processing incremental changes.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_32.png\"></img>\n",
    "To facilitate the demonstration of structured streams, we are using **`trigger(availableNow=True)`** to slow\n",
    "down the processing of the data combined with **`query.awaitTermination()`** to prevent the lesson from\n",
    "moving forward until the one batch is processed.  **trigger-available-now** is very similar to **trigger-once** but can run\n",
    "multiple batches until all available data is consumed instead of once big batch and is introduced in\n",
    "<a href=\"https://spark.apache.org/releases/spark-release-3-3-0.html\" target=\"_blank\">Spark 3.3.0</a> and\n",
    "<a href=\"https://docs.databricks.com/release-notes/runtime/10.4.html\" target=\"_blank\">Databricks Runtime 10.4 LTS</a>."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "9884df31-b541-4196-80ca-9505e1316619",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "update_silver()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "2239a741-3eab-49bf-afb5-b051e270a0e2",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "As expected, the stream runs for a very brief time, and the **`silver`** table written contains all the values previously written to **`bronze`**."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "fa2419cd-8896-4887-b983-d75049395fab",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT * FROM silver\n",
    "ORDER BY processed_time DESC, id DESC"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "fc6edd77-ae37-4429-a34f-69042696c469",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Processing new records is as easy as adding them to our source table **`bronze`**..."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "ac72544c-6050-4c02-84ad-7468c1a01821",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "INSERT INTO bronze\n",
    "VALUES (4, \"Ted\", 4.7),\n",
    "       (5, \"Tiffany\", 5.5),\n",
    "       (6, \"Vini\", 6.3)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "5034dc12-a901-4434-b1f5-b19b159367e3",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "... and re-executing the incremental batch processing code."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "630dbbb0-c666-4520-86a9-758c3c9537ac",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "update_silver()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "6e193601-5b19-4613-9bbd-76f045fa1aeb",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Delta Lake is ideally suited for easily tracking and propagating inserted data through a series of tables. This pattern has a number of names, including \"medallion\", \"multi-hop\", \"Delta\", and \"bronze/silver/gold\" architecture."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "2261884f-5574-4b9b-abce-567c6b3c7678",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT * FROM silver\n",
    "ORDER BY processed_time DESC, id DESC"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "e97efe30-3b18-4f5d-a8dd-ead5ee579b49",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Writing to Multiple Tables\n",
    "\n",
    "Those familiar with Structured Streaming may be aware that the **`foreachBatch`** method provides the option to execute custom data writing logic on each microbatch of streaming data.\n",
    "\n",
    "The Databricks Runtime provides guarantees that these <a href=\"https://docs.databricks.com/delta/delta-streaming.html#idempot-write\" target=\"_blank\">streaming Delta Lake writes will be idempotent</a>, even when writing to multiple tables, IF you set the \"txnVersion\" and \"txnAppId\" options. This is especially useful when data for multiple tables might be contained within a single record.  This was added in <a href=\"https://docs.databricks.com/release-notes/runtime/8.4.html\" target=\"_blank\">Databricks Runtime 8.4</a>.\n",
    "\n",
    "The code below first defines the custom writer logic to append records to two new tables, and then demonstrates using this function within **`foreachBatch`**.\n",
    "\n",
    "There is some debate as to whether you should use foreachBatch to write to multiple tables or to simply use multiple streams.  Generally multiple streams is the simpler and more efficient design because it allows streaming jobs writing to each table to run independently of each other.  Whereas using foreachBatch to write to multiple tables has the advantage of keeping writes to the two tables in sync."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "b01de844-932a-4466-a2f6-ba67103535c9",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def write_twice(microBatchDF, batchId):\n",
    "    appId = \"write_twice\"\n",
    "    \n",
    "    microBatchDF.select(\"id\", \"name\", F.current_timestamp().alias(\"processed_time\")).write.option(\"txnVersion\", batchId).option(\"txnAppId\", appId).mode(\"append\").saveAsTable(\"silver_name\")\n",
    "    \n",
    "    microBatchDF.select(\"id\", \"value\", F.current_timestamp().alias(\"processed_time\")).write.option(\"txnVersion\", batchId).option(\"txnAppId\", appId).mode(\"append\").saveAsTable(\"silver_value\")\n",
    "\n",
    "\n",
    "def split_stream():\n",
    "    query = (spark.readStream.table(\"bronze\")\n",
    "                 .writeStream\n",
    "                 .foreachBatch(write_twice)\n",
    "                 .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/split_stream\")\n",
    "                 .trigger(availableNow=True)\n",
    "                 .start())\n",
    "    \n",
    "    query.awaitTermination()\n",
    "    "
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "7bfb61bb-3d72-4f72-9e4d-45136eaa876d",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that while a stream will again be triggered, the two writes contained within the **`write_twice`** function are using Spark batch syntax. This will always be the case for writers called by **`foreachBatch`**."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "50b1cce8-74fb-43ee-a1f7-add762ea0fe4",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "split_stream()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "cd6965d5-44ba-4617-aa67-235ee3984b2e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "The cells below demonstrate the logic was applied properly to split the initial data into two tables."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "aadb091d-60d7-45fa-8d85-a0caa002da6f",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT * FROM silver_name\n",
    "ORDER BY processed_time DESC, id DESC"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "4bb48c90-19b0-4f0a-b1a2-1299cd667c4d",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the **`processed_time`** for each of these tables differs slightly. The logic defined above captures the current timestamp at the time each write executes, demonstrating that while both writes happen within the same streaming microbatch process, they are fully independent transactions (as such, downstream logic should be tolerant for slightly asynchronous updates)."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "5562e181-2eb3-4634-9464-ea5c0bd10ff6",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT * FROM silver_value\n",
    "ORDER BY processed_time DESC, id DESC"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "c8db83f9-4f79-495e-b3c0-bcb4bb1521c8",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Insert more values into the **`bronze`** table."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "2fec33c4-4159-4b81-b597-88217b7f9e55",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "INSERT INTO bronze\n",
    "VALUES (7, \"Viktor\", 7.4),\n",
    "       (8, \"Hiro\", 8.2),\n",
    "       (9, \"Shana\", 9.9)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "0e7acebf-6d50-42c0-bc75-88593e6260d6",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "And we can now pick up these new records and write to two tables."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "dbeb66e5-b0df-4d2b-8644-5fd825a11346",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "split_stream()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "35471a0d-a485-48d6-ac11-56aecb33cee3",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "As expected, only new values are inserted into the two tables, again a few moments apart."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "d20595d9-d137-45ff-a78f-b8bcb6115de5",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT * FROM silver_name\n",
    "ORDER BY processed_time DESC, id DESC"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "97ffff03-4200-40d0-80ae-545842d48eb2",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT * FROM silver_value\n",
    "ORDER BY processed_time DESC, id DESC"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "d17b0038-bba1-42f6-8b46-b36400c0344a",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Update Aggregates in a Key-Value Store\n",
    "\n",
    "Incremental aggregation can be useful for a number of purposes, including dashboarding and enriching reports with current summary data.\n",
    "\n",
    "The logic below defines a handful of aggregations against the **`silver`** table."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "a18e151e-935c-4a39-957c-d8d4cf880a22",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def update_key_value():\n",
    "    query = (spark.readStream\n",
    "                  .table(\"silver\")\n",
    "                  .groupBy(\"id\")\n",
    "                  .agg(F.sum(\"value\").alias(\"total_value\"), \n",
    "                       F.mean(\"value\").alias(\"avg_value\"),\n",
    "                       F.count(\"value\").alias(\"record_count\"))\n",
    "                  .writeStream\n",
    "                  .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/key_value\")\n",
    "                  .outputMode(\"complete\")\n",
    "                  .trigger(availableNow=True)\n",
    "                  .table(\"key_value\"))\n",
    "    \n",
    "    query.awaitTermination()\n",
    "    "
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "acb78839-673f-4464-9efa-5f45a96557a7",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "**NOTE**: Because the transformations above require shuffling data, setting the number of shuffle partitions to the maximum number of cores will provide more efficient performance. \n",
    "\n",
    "The default number of shuffle partitions (200) can cripple many streaming jobs.\n",
    "\n",
    "As such, it's a reasonably good practice to simply use the maximum number of cores as the high end, and if smaller, maintain a factor of the number of cores.\n",
    "\n",
    "Naturally, this generalized advice changes as you increase the number of streams running on a single cluster."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "66e930af-b9f7-419b-8f32-ee083d18ca3b",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"Executor cores: {sc.defaultParallelism}\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", sc.defaultParallelism)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "04cc63a6-dbfa-4131-a94b-8c5f1e29868e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "update_key_value()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "415af718-a3f3-4e8f-a50c-3c492ad853b2",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT * FROM key_value\n",
    "ORDER BY id"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "0faa3df4-188a-410f-88fc-29acef6715ab",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adding more values to the **`silver`** table will allow more interesting aggregation."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "91385ed3-1047-444f-a4c6-2ebd1d6b5763",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "INSERT INTO silver\n",
    "VALUES (1, \"Yve\", 1.0, current_timestamp()),\n",
    "       (2, \"Omar\", 2.5, current_timestamp()),\n",
    "       (3, \"Elia\", 3.3, current_timestamp()),\n",
    "       (7, \"Viktor\", 7.4, current_timestamp()),\n",
    "       (8, \"Hiro\", 8.2, current_timestamp()),\n",
    "       (9, \"Shana\", 9.9, current_timestamp())"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "0076d826-59f4-46cb-b186-cf737a6fc8d2",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "One thing to note is that the logic being executed is currently overwriting the resulting table with each write. In the next section, **`MERGE`** will be used in combination with **`foreachBatch`** to update existing records. This pattern can also be applied with key-value stores."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "b94a9182-03ee-4bc5-9356-f529576fb423",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "update_key_value()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "e2d89256-b6fb-4162-86c3-67efe1276486",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT * FROM key_value\n",
    "ORDER BY id"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "84f8b66d-c749-4018-8101-cb9791a6726f",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing Change Data Capture Data\n",
    "While the change data capture (CDC) data emitted by various systems will vary greatly, incrementally processing these data with Databricks is straightforward.\n",
    "\n",
    "Here the **`bronze_status`** table will represent the raw CDC information, rather than row-level data."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "b6bb9c40-a367-49b3-aba0-e98467e5aa4e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "CREATE TABLE bronze_status \n",
    "(user_id INT, status STRING, update_type STRING, processed_timestamp TIMESTAMP);\n",
    "\n",
    "INSERT INTO bronze_status\n",
    "VALUES  (1, \"new\", \"insert\", current_timestamp()),\n",
    "        (2, \"repeat\", \"update\", current_timestamp()),\n",
    "        (3, \"at risk\", \"update\", current_timestamp()),\n",
    "        (4, \"churned\", \"update\", current_timestamp()),\n",
    "        (5, null, \"delete\", current_timestamp())"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "c16a00e8-26d7-469b-bcc1-87f3b27b6af1",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "The **`silver_status`** table below has been created to track the current **`status`** for a given **`user_id`**."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "8ea2b1f0-7146-4219-89ce-4f2cf58a98f0",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "CREATE TABLE silver_status (user_id INT, status STRING, updated_timestamp TIMESTAMP)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "47a65de2-609e-45b9-a1d1-f733005eb321",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "The **`MERGE`** statement can easily be written with SQL to apply CDC changes appropriately, given the type of update received.\n",
    "\n",
    "The rest of the **`upsert_cdc`** method contains the logic necessary to run SQL code against a micro-batch in a PySpark DataStreamWriter."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "99431ae7-71e3-46d7-b88d-b7bcc2ede3ec",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def upsert_cdc(microBatchDF, batchID):\n",
    "    microBatchDF.createTempView(\"bronze_batch\")\n",
    "    \n",
    "    query = \"\"\"\n",
    "        MERGE INTO silver_status s\n",
    "        USING bronze_batch b\n",
    "        ON b.user_id = s.user_id\n",
    "        WHEN MATCHED AND b.update_type = \"update\"\n",
    "          THEN UPDATE SET user_id=b.user_id, status=b.status, updated_timestamp=b.processed_timestamp\n",
    "        WHEN MATCHED AND b.update_type = \"delete\"\n",
    "          THEN DELETE\n",
    "        WHEN NOT MATCHED AND b.update_type = \"update\" OR b.update_type = \"insert\"\n",
    "          THEN INSERT (user_id, status, updated_timestamp)\n",
    "          VALUES (b.user_id, b.status, b.processed_timestamp)\n",
    "    \"\"\"\n",
    "    \n",
    "    microBatchDF._jdf.sparkSession().sql(query)\n",
    "    \n",
    "def streaming_merge():\n",
    "    query = (spark.readStream\n",
    "                  .table(\"bronze_status\")\n",
    "                  .writeStream\n",
    "                  .foreachBatch(upsert_cdc)\n",
    "                  .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/silver_status\")\n",
    "                  .trigger(availableNow=True)\n",
    "                  .start())\n",
    "    \n",
    "    query.awaitTermination()\n",
    "    "
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "51fc5a7e-8fb6-4b5a-8254-6bafd59d3bcb",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "As always, we incrementally process newly arriving records."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "a2a55b5b-4736-4495-a131-60fd7051744b",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "streaming_merge()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "406668d0-894f-4c26-bba3-317c6fbc8e7f",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT * FROM silver_status\n",
    "ORDER BY updated_timestamp DESC, user_id DESC"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "9f067d54-79cb-4817-911c-d1c8027a965e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inserting new records will allow us to then apply these changes to our silver data."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "32647921-fdb0-4427-aacb-efbc1f9dd9a5",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "INSERT INTO bronze_status\n",
    "VALUES  (1, \"repeat\", \"update\", current_timestamp()),\n",
    "        (2, \"at risk\", \"update\", current_timestamp()),\n",
    "        (3, \"churned\", \"update\", current_timestamp()),\n",
    "        (4, null, \"delete\", current_timestamp()),\n",
    "        (6, \"new\", \"insert\", current_timestamp())"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "b3a41ba5-557d-4c7d-8703-357d38af6537",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "streaming_merge()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "76491d58-904e-4017-8346-da26ed351b80",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that at present, the logic would not be particularly robust to data arriving out-of-order or duplicate records (but these occurences can be handled)."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "f550a80f-9237-48e0-b37c-58bafcdfc9b5",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT * FROM silver_status\n",
    "ORDER BY updated_timestamp DESC, user_id DESC"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "c82eec45-d93e-4477-ba2d-67e453dfc86d",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Joining Two Incremental Tables\n",
    "\n",
    "Note that there are many intricacies around watermarking and windows when dealing with incremental joins, and that not all join types are supported."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "944d5a9f-ca85-4d6f-9f36-789505ca129f",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def stream_stream_join():\n",
    "    nameDF = spark.readStream.table(\"silver_name\")\n",
    "    valueDF = spark.readStream.table(\"silver_value\")\n",
    "    \n",
    "    return (nameDF.join(valueDF, nameDF.id == valueDF.id, \"inner\")\n",
    "                  .select(nameDF.id, \n",
    "                          nameDF.name, \n",
    "                          valueDF.value, \n",
    "                          F.current_timestamp().alias(\"joined_timestamp\"))\n",
    "                  .writeStream\n",
    "                  .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/joined\")\n",
    "                  .queryName(\"joined_streams_query\")\n",
    "                  .table(\"joined_streams\")\n",
    "           )"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "7e0dd65e-f7f1-4366-9fb5-dc674730de2e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the logic defined above does not set a **`trigger`** option.\n",
    "\n",
    "This means that the stream will run in continuous execution mode, triggering every 500ms by default."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "5909d1cf-0aa2-423b-871d-e4510c27b54e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "query = stream_stream_join()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "5e3032e1-625d-4dbf-98da-edbea5880e76",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "This also means that it is possible to read the new tables before there is any data in them.\n",
    "\n",
    "Because the stream never stops we can't block until the trigger-available-now stream has terminated with **`awaitTermination()`**.\n",
    "\n",
    "Instead we can block until \"some\" data is processed by leveraging **`query.recentProgress`**."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "7021c4e6-2e27-42be-beef-ea660203ae27",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def block_until_stream_is_ready(query, min_batches=2):\n",
    "    import time\n",
    "    while len(query.recentProgress) < min_batches:\n",
    "        time.sleep(5) # Give it a couple of seconds\n",
    "\n",
    "    print(f\"The stream has processed {len(query.recentProgress)} batchs\")\n",
    "    \n",
    "block_until_stream_is_ready(query)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "989454e7-0fa1-41e3-8a67-60a0c420eca7",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Running **`display()`** on a streaming table is a way to monitor table updates in near-real-time while in interactive development."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "8c2ba88a-3d0d-47c1-9b24-c211fccb5f54",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "display(spark.readStream.table(\"joined_streams\"))"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "b0b070af-aac2-42eb-967c-5b611649f667",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"https://files.training.databricks.com/images/icon_note_32.png\"> Anytime a streaming read is displayed to a notebook, a streaming job will begin.\n",
    "\n",
    "Here a second stream is started.  One is processing the data as part of our original pipline, and now a second streaming job is running to update the **`display()`** function with the latest results."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "f7378bc4-e6d8-4249-b48b-2ab033f8d456",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for stream in spark.streams.active:\n",
    "    print(stream.name)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "0da8d4cf-66b4-47ab-8d94-5b27d7873538",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we'll add new values to the **`bronze`** table."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "48203862-da5b-4e3a-bc3e-45c4b0205622",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "INSERT INTO bronze\n",
    "VALUES (10, \"Pedro\", 10.5),\n",
    "       (11, \"Amelia\", 11.5),\n",
    "       (12, \"Diya\", 12.3),\n",
    "       (13, \"Li\", 13.4),\n",
    "       (14, \"Daiyu\", 14.2),\n",
    "       (15, \"Jacques\", 15.9)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "003e0607-616b-42a1-8763-15eaf6141a1c",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "The stream-stream join is configured against the tables resulting from the **`split_stream`** function; run this again and data should quickly process through the streaming join running above."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "87bbfd86-c4d8-4b75-b480-4a412ca8fe72",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "split_stream()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "010dc23c-3222-446c-8412-66e45da4fd7f",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Interactive streams should always be stopped before leaving a notebook session, as they can keep clusters from auto-terminating and incur unnecessary cloud costs.  \n",
    "You can terminate a streaming job by clicking \"Cancel\" on a running cell, \"Stop Execution\" at the top of the notebook, or by running the code below."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "9a7233d5-3cc7-473d-95e2-5e0824fdff6a",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for stream in spark.streams.active:\n",
    "    print(f\"Stopping {stream.name}\")\n",
    "    stream.stop()\n",
    "    stream.awaitTermination()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "d42c7761-066b-45c6-a00a-525bd2afb2f2",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Join Incremental and Static Data\n",
    "\n",
    "While incremental tables are ever-appending, static tables typically can be thought of as containing data that may be changed or overwritten.\n",
    "\n",
    "Because of Delta Lake's transactional guarantees and caching, Databricks ensures that each microbatch of streaming data that's joined back to a static table will contain the current version of data from the static table."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "f17224af-a195-4e6a-aa2f-a4c4778dcbb6",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "statusDF = spark.read.table(\"silver_status\")\n",
    "bronzeDF = spark.readStream.table(\"bronze\")\n",
    "\n",
    "query = (bronzeDF.alias(\"bronze\")\n",
    "                 .join(statusDF.alias(\"status\"), bronzeDF.id==statusDF.user_id, \"inner\")\n",
    "                 .select(\"bronze.*\", \"status.status\")\n",
    "                 .writeStream\n",
    "                 .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/join_status\")\n",
    "                 .queryName(\"joined_status_query\")\n",
    "                 .table(\"joined_status\")\n",
    ")"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "c09d8722-a107-4ce4-af32-fb9cc22a05a6",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again, wait until we have some data before moving forward"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "da9312a4-da6f-47f5-baee-0a6dfcc701b3",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "block_until_stream_is_ready(query)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "3a9aab4f-a07a-48be-8d34-4842a80eaf5e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT * FROM joined_status\n",
    "ORDER BY id DESC"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "a4f07e00-3855-4405-bcd2-3231be66cc08",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Only those records with a matching **`id`** in **`joined_status`** at the time the stream is processed will be represented in the resulting table."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "9afc2e6c-4d2f-4669-afe7-b01aea6b7698",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT * FROM silver_status\n",
    "ORDER BY updated_timestamp DESC, user_id DESC"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "b48add93-dda6-4629-aabb-bda00807a3df",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Processing new records into the **`silver_status`** table will not automatically trigger updates to the results of the stream-static join."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "9c3ff405-7c9b-4d03-88c5-c2cabb5c63c2",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "INSERT INTO bronze_status\n",
    "VALUES  (11, \"repeat\", \"update\", current_timestamp()),\n",
    "        (12, \"at risk\", \"update\", current_timestamp()),\n",
    "        (16, \"new\", \"insert\", current_timestamp()),\n",
    "        (17, \"repeat\", \"update\", current_timestamp())"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "e112153f-b4b3-45d6-b08b-5caaa5601a49",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "streaming_merge()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "d6bc0ed0-4a71-4e23-9ce0-30955b6b883b",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT * FROM joined_status\n",
    "ORDER BY id DESC"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "d5ce0781-b814-4f74-8bf3-a672498028ee",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Only new data appearing on the streaming side of the query will trigger records to process using this pattern."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "600353e4-0c95-49a3-9305-e0fcf87b38f8",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "INSERT INTO bronze\n",
    "VALUES (16, \"Marissa\", 1.9),\n",
    "       (17, \"Anne\", 2.7)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "36c2a115-0c49-4b59-b3d5-7f7ea4a8a002",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "The incremental data in a stream-static join \"drives\" the stream, guaranteeing that each microbatch of data joins with the current values present in the valid version of the static table."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "49147a82-caf1-40f5-9530-0037ec189899",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT * FROM joined_status\n",
    "ORDER BY id DESC"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "8fcceb86-6f1e-44f8-81b7-a5cdb9009a28",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the following cell to delete the tables and files associated with this lesson."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "3679809f-6183-4b6f-bbed-ff91d2db3db0",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "DA.cleanup()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "efb3bcf3-ecc3-4682-afd3-e2092ad77c81",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "notebookName": "ADE 1.1 - Streaming Design Patterns",
   "dashboards": [],
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "language": "python",
   "widgets": {},
   "notebookOrigID": 4185566132530773
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}