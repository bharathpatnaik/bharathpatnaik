{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DataFrames and Transformations Review\n",
    "## De-Duping Data Lab\n",
    "\n",
    "In this exercise, we're doing ETL on a file we've received from a customer. That file contains data about people, including:\n",
    "\n",
    "* first, middle and last names\n",
    "* gender\n",
    "* birth date\n",
    "* Social Security number\n",
    "* salary\n",
    "\n",
    "But, as is unfortunately common in data we get from this customer, the file contains some duplicate records. Worse:\n",
    "\n",
    "* In some of the records, the names are mixed case (e.g., \"Carol\"), while in others, they are uppercase (e.g., \"CAROL\").\n",
    "* The Social Security numbers aren't consistent either. Some of them are hyphenated (e.g., \"992-83-4829\"), while others are missing hyphens (\"992834829\").\n",
    "\n",
    "If all of the name fields match -- if you disregard character case -- then the birth dates and salaries are guaranteed to match as well,\n",
    "and the Social Security Numbers *would* match if they were somehow put in the same format.\n",
    "\n",
    "Your job is to remove the duplicate records. The specific requirements of your job are:\n",
    "\n",
    "* Remove duplicates. It doesn't matter which record you keep; it only matters that you keep one of them.\n",
    "* Preserve the data format of the columns. For example, if you write the first name column in all lowercase, you haven't met this requirement.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_hint_32.png\" alt=\"Hint\"> The initial dataset contains 103,000 records.\n",
    "The de-duplicated result has 100,000 records.\n",
    "\n",
    "Next, write the results in **Delta** format as a **single data file** to the directory given by the variable *deltaDestDir*.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_hint_32.png\" alt=\"Hint\"> Remember the relationship between the number of partitions in a DataFrame and the number of files written.\n",
    "\n",
    "##### Methods\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#input-and-output\" target=\"_blank\">DataFrameReader</a>\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html\" target=\"_blank\">DataFrame</a>\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html?#functions\" target=\"_blank\">Built-In Functions</a>\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#input-and-output\" target=\"_blank\">DataFrameWriter</a>"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "d695a128-7069-4b7a-a16c-da84c37131f2",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%run ./Includes/Classroom-Setup"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "3eb23ade-dfc8-4a2e-9728-5d25226ba20d",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "It's helpful to look at the file first, so you can check the format. `dbutils.fs.head()` (or just `%fs head`) is a big help here."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "3202dc74-cea8-4094-9c66-3561472fcf72",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%fs head dbfs:/mnt/training/dataframes/people-with-dups.txt"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "68030557-d31e-45d2-b963-f7e683204e90",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "# ANSWER\n",
    "\n",
    "sourceFile = \"dbfs:/mnt/training/dataframes/people-with-dups.txt\"\n",
    "deltaDestDir = workingDir + \"/people\"\n",
    "\n",
    "# In case it already exists\n",
    "dbutils.fs.rm(deltaDestDir, True)\n",
    "\n",
    "# dropDuplicates() will introduce a shuffle, so it helps to reduce the number of post-shuffle partitions.\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 8)\n",
    "\n",
    "# Okay, now we can read this thing\n",
    "df = (spark\n",
    "      .read\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .option(\"sep\", \":\")\n",
    "      .csv(sourceFile)\n",
    "     )"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "0d45604f-2130-4072-a93b-cdbf6ba45afa",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "# ANSWER\n",
    "from pyspark.sql.functions import col, lower, translate\n",
    "\n",
    "dedupedDF = (df\n",
    "             .select(col(\"*\"),\n",
    "                     lower(col(\"firstName\")).alias(\"lcFirstName\"),\n",
    "                     lower(col(\"lastName\")).alias(\"lcLastName\"),\n",
    "                     lower(col(\"middleName\")).alias(\"lcMiddleName\"),\n",
    "                     translate(col(\"ssn\"), \"-\", \"\").alias(\"ssnNums\")\n",
    "                     # regexp_replace(col(\"ssn\"), \"-\", \"\").alias(\"ssnNums\")  # An alternate function to strip the hyphens\n",
    "                     # regexp_replace(col(\"ssn\"), \"\"\"^(\\d{3})(\\d{2})(\\d{4})$\"\"\", \"$1-$2-$3\").alias(\"ssnNums\")  # An alternate that adds hyphens if missing\n",
    "                    )\n",
    "             .dropDuplicates([\"lcFirstName\", \"lcMiddleName\", \"lcLastName\", \"ssnNums\", \"gender\", \"birthDate\", \"salary\"])\n",
    "             .drop(\"lcFirstName\", \"lcMiddleName\", \"lcLastName\", \"ssnNums\")\n",
    "            )"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "31e405c9-a7c3-4078-a89a-d6a973ab6b9f",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "# ANSWER\n",
    "\n",
    "# Now, write the results in Delta format as a single file. We'll also display the Delta files to make sure they were written as expected.\n",
    "\n",
    "(dedupedDF\n",
    " .repartition(1)\n",
    " .write\n",
    " .mode(\"overwrite\")\n",
    " .format(\"delta\")\n",
    " .save(deltaDestDir)\n",
    ")\n",
    "\n",
    "display(dbutils.fs.ls(deltaDestDir))"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "d2da8b6f-b4bd-47e9-9cdf-ee2dc87abded",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "**CHECK YOUR WORK**"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "0e69e0f8-7a29-4519-8426-49d4ce615378",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "verify_files = dbutils.fs.ls(deltaDestDir)\n",
    "verify_delta_format = False\n",
    "verify_num_data_files = 0\n",
    "for f in verify_files:\n",
    "    if f.name == '_delta_log/':\n",
    "        verify_delta_format = True\n",
    "    elif f.name.endswith('.parquet'):\n",
    "        verify_num_data_files += 1\n",
    "\n",
    "assert verify_delta_format, \"Data not written in Delta format\"\n",
    "assert verify_num_data_files == 1, \"Expected 1 data file written\"\n",
    "\n",
    "verify_record_count = spark.read.format(\"delta\").load(deltaDestDir).count()\n",
    "assert verify_record_count == 100000, \"Expected 100000 records in final result\"\n",
    "\n",
    "del verify_files, verify_delta_format, verify_num_data_files, verify_record_count"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "fbf3693e-0f99-4005-a6ae-68fdf1f43bb4",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clean up classroom\n",
    "Run the cell below to clean up resources."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "1387a430-a509-4fa3-acb3-476a07d32d8b",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%run \"./Includes/Classroom-Cleanup\""
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "244c1177-1104-4e1a-9b42-b0b2dbad6329",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "notebookName": "ASP 3.4 - Review",
   "dashboards": [],
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "language": "python",
   "widgets": {},
   "notebookOrigID": 4185566132528029
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}