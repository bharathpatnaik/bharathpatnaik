{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Aggregating Streams\n",
    "\n",
    "##### Objectives\n",
    "1. Add watermarking\n",
    "1. Aggregate with windows\n",
    "1. Display streaming query results\n",
    "1. Monitor streaming queries\n",
    "\n",
    "##### Classes\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.streaming.DataStreamReader.html#pyspark.sql.streaming.DataStreamReader\" target=\"_blank\">DataStreamReader</a>\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.streaming.DataStreamWriter.html#pyspark.sql.streaming.DataStreamWriter\" target=\"_blank\">DataStreamWriter</a>\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.streaming.StreamingQuery.html#pyspark.sql.streaming.StreamingQuery\" target=\"_blank\">StreamingQuery</a>\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.streaming.StreamingQueryManager.html#pyspark.sql.streaming.StreamingQueryManager\" target=\"_blank\">StreamingQueryManager</a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hourly Activity by Traffic Lab\n",
    "Process streaming data to display the total active users by traffic source with a 1 hour window.\n",
    "1. Cast to timestamp and add watermark for 2 hours\n",
    "2. Aggregate active users by traffic source for 1 hour windows\n",
    "3. Execute query with `display` and plot results\n",
    "5. Use query name to stop streaming query"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "02a16960-38ad-43f7-9a37-7ac50593568d",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup\n",
    "Run the cells below to generate hourly JSON files of event data for July 3, 2020."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "ba4dfd9e-a4f5-41b4-bc60-0d2713176790",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%run ./Includes/Classroom-Setup"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "1b3d733b-6b8d-4deb-98da-39e1c3fef658",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "schema = \"device STRING, ecommerce STRUCT<purchase_revenue_in_usd: DOUBLE, total_item_quantity: BIGINT, unique_items: BIGINT>, event_name STRING, event_previous_timestamp BIGINT, event_timestamp BIGINT, geo STRUCT<city: STRING, state: STRING>, items ARRAY<STRUCT<coupon: STRING, item_id: STRING, item_name: STRING, item_revenue_in_usd: DOUBLE, price_in_usd: DOUBLE, quantity: BIGINT>>, traffic_source STRING, user_first_touch_timestamp BIGINT, user_id STRING\"\n",
    "\n",
    "# Directory of hourly events logged from the BedBricks website on July 3, 2020\n",
    "hourlyEventsPath = \"/mnt/training/ecommerce/events/events-2020-07-03.json\"\n",
    "\n",
    "df = (spark\n",
    "      .readStream\n",
    "      .schema(schema)\n",
    "      .option(\"maxFilesPerTrigger\", 1)\n",
    "      .json(hourlyEventsPath)\n",
    "     )"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "7febf6d2-0694-400b-9283-6d6cf4e90871",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Cast to timestamp and add watermark for 2 hours\n",
    "- Add a **`createdAt`** column by dividing **`event_timestamp`** by 1M and casting to timestamp\n",
    "- Set a watermark of 2 hours on the **`createdAt`** column\n",
    "\n",
    "Assign the resulting DataFrame to **`eventsDF`**."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "ec1f60bc-86f0-4599-bd13-b8f9c10f93fe",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ANSWER\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "eventsDF = (df\n",
    "            .withColumn(\"createdAt\", (col(\"event_timestamp\") / 1e6).cast(\"timestamp\"))\n",
    "            .withWatermark(\"createdAt\", \"2 hours\")\n",
    "           )"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "8619f545-0328-4dd4-a829-69421695a4c1",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "**CHECK YOUR WORK**"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "90c8ab0a-de8a-4a2b-8c49-4f34d2ed9ed9",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "assert \"StructField(createdAt,TimestampType,true\" in str(eventsDF.schema)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "f1fbf213-77ba-4c49-b569-1492d983c014",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Aggregate active users by traffic source for 1 hour windows\n",
    "- Set the default shuffle partitions to the number of cores on your cluster (not required, but runs faster)\n",
    "- Group by **`traffic_source`** with 1-hour tumbling windows based on the **`createdAt`** column\n",
    "- Aggregate the approximate count of distinct users per **`traffic_source`** and hour, and alias the column to \"active_users\"\n",
    "- Select **`traffic_source`**, **`active_users`**, and the **`hour`** extracted from **`window.start`** with an alias of \"hour\"\n",
    "- Sort by **`hour`** in ascending order\n",
    "\n",
    "Assign the resulting DataFrame to **`trafficDF`**."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "2e6f4bd6-a836-4324-b375-f32c697bf576",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ANSWER\n",
    "from pyspark.sql.functions import approx_count_distinct, hour, window\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)\n",
    "\n",
    "trafficDF = (eventsDF\n",
    "             .groupBy(\"traffic_source\", window(col(\"createdAt\"), \"1 hour\"))\n",
    "             .agg(approx_count_distinct(\"user_id\").alias(\"active_users\"))\n",
    "             .select(col(\"traffic_source\"), col(\"active_users\"), hour(col(\"window.start\")).alias(\"hour\"))\n",
    "             .sort(\"hour\")\n",
    "            )"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "89382ef0-969b-4423-9af1-88d7e54cdc0a",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "**CHECK YOUR WORK**"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "6dc38fd6-24db-4717-b53f-688e87cdfd97",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "assert str(trafficDF.schema) == \"StructType(List(StructField(traffic_source,StringType,true),StructField(active_users,LongType,false),StructField(hour,IntegerType,true)))\""
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "b2eb4909-98b9-45d9-a786-5d593ff6424f",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Execute query with display() and plot results\n",
    "- Use `display` to start **`trafficDF`** as a streaming query and display the resulting memory sink\n",
    "  - Assign \"hourly_traffic\" as the name of the query by seting the **`streamName`** parameter of `display`\n",
    "- Plot the streaming query results as a bar graph\n",
    "- Configure the following plot options:\n",
    "  - Keys: **`hour`**\n",
    "  - Series groupings: **`traffic_source`**\n",
    "  - Values: **`active_users`**"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "11e3b8ac-54ee-49d5-93a2-63982b725f1a",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ANSWER\n",
    "display(trafficDF, streamName=\"hourly_traffic\")"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "4495878b-a25f-4468-bba1-f8a0f5fc4ae4",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "**CHECK YOUR WORK**\n",
    "\n",
    "- The bar chart should plot `hour` on the x-axis and `active_users` on the y-axis\n",
    "- Six bars should appear at every hour for all traffic sources\n",
    "- The chart should stop at hour 23"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "24be2085-1161-405f-8cca-0a8da342a3d2",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Manage streaming query\n",
    "- Iterate over SparkSession's list of active streams to find one with name \"hourly_traffic\"\n",
    "- Stop the streaming query"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "bdfc4184-02ce-41fc-a66c-4ae3cde38166",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ANSWER\n",
    "untilStreamIsReady(\"hourly_traffic\")\n",
    "\n",
    "for s in spark.streams.active:\n",
    "    if s.name == \"hourly_traffic\":\n",
    "        s.stop()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "5d2fcfc0-ed0a-4e97-ae63-8608dc8382e9",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "%md **CHECK YOUR WORK**  \n",
    "Print all active streams to check that \"hourly_traffic\" is no longer there"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "4ab9bf10-dda5-449c-a6e3-165daadbaef1",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for s in spark.streams.active:\n",
    "    print(s.name)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "c86ecb29-695c-43c4-8db9-a1123aa10806",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classroom Cleanup\n",
    "Run the cell below to clean up resources."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "96e5e386-9d8f-467c-bafe-19ba969965db",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%run ./Includes/Classroom-Cleanup"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "f66799a3-4217-471a-8651-55c596816bc2",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "notebookName": "ASP 4.2 - Aggregating Streams",
   "dashboards": [],
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "language": "python",
   "widgets": {},
   "notebookOrigID": 4185566132527847
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}