{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Query Optimization\n",
    "\n",
    "We'll explore query plans and optimizations for several examples including logical optimizations and exanples with and without predicate pushdown.\n",
    "\n",
    "##### Objectives\n",
    "1. Logical optimizations\n",
    "1. Predicate pushdown\n",
    "1. No predicate pushdown\n",
    "\n",
    "##### Methods\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html\" target=\"_blank\">DataFrame</a>: `explain`"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "df6aacce-8d74-4a03-8812-18287fb94f3d",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![query optimization](https://files.training.databricks.com/images/aspwd/query_optimization_catalyst.png)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "49ae2c5e-232a-4e3c-8039-7ec78bb5724c",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![query optimization aqe](https://files.training.databricks.com/images/aspwd/query_optimization_aqe.png)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "f9d3ae84-5067-447c-9431-d3d235ac5441",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Letâ€™s run our set up cell, and get our initial DataFrame stored in the variable `df`. Displaying this DataFrame shows us events data."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "1e8e9709-f847-487b-9bc4-8714a9595db6",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%run ./Includes/Classroom-Setup"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "70c5904d-3b55-451d-a607-eb700515601c",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "df = spark.read.parquet(eventsPath)\n",
    "display(df)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "9eb0502c-6d8d-48f3-b33f-1e62ac5f682f",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Logical Optimization\n",
    "\n",
    "`explain(..)` prints the query plans, optionally formatted by a given explain mode. Compare the following logical plan & physical plan, noting how Catalyst handled the multiple `filter` transformations."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "fbff94da-49dd-452d-9824-dbb2b0d96818",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "limitEventsDF = (df\n",
    "                 .filter(col(\"event_name\") != \"reviews\")\n",
    "                 .filter(col(\"event_name\") != \"checkout\")\n",
    "                 .filter(col(\"event_name\") != \"register\")\n",
    "                 .filter(col(\"event_name\") != \"email_coupon\")\n",
    "                 .filter(col(\"event_name\") != \"cc_info\")\n",
    "                 .filter(col(\"event_name\") != \"delivery\")\n",
    "                 .filter(col(\"event_name\") != \"shipping_info\")\n",
    "                 .filter(col(\"event_name\") != \"press\")\n",
    "                )\n",
    "\n",
    "limitEventsDF.explain(True)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "7e3a28d5-05a1-4864-bbdd-22cbbd6f35a0",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Of course, we could have written the query originally using a single `filter` condition ourselves. Compare the previous and following query plans."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "bbf6506d-e914-4045-bdb5-187d4af8e731",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "betterDF = (df\n",
    "            .filter((col(\"event_name\").isNotNull()) &\n",
    "                    (col(\"event_name\") != \"reviews\") &\n",
    "                    (col(\"event_name\") != \"checkout\") &\n",
    "                    (col(\"event_name\") != \"register\") &\n",
    "                    (col(\"event_name\") != \"email_coupon\") &\n",
    "                    (col(\"event_name\") != \"cc_info\") &\n",
    "                    (col(\"event_name\") != \"delivery\") &\n",
    "                    (col(\"event_name\") != \"shipping_info\") &\n",
    "                    (col(\"event_name\") != \"press\"))\n",
    "           )\n",
    "\n",
    "betterDF.explain(True)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "4904d0d6-b84b-4cbd-846a-4b70f0e31787",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Of course, we wouldn't write the following code intentionally, but in a long, complex query you might not notice the duplicate filter conditions. Let's see what Catalyst does with this query."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "2e3b83c7-a090-4136-ad06-633e20c0ad8f",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "stupidDF = (df\n",
    "            .filter(col(\"event_name\") != \"finalize\")\n",
    "            .filter(col(\"event_name\") != \"finalize\")\n",
    "            .filter(col(\"event_name\") != \"finalize\")\n",
    "            .filter(col(\"event_name\") != \"finalize\")\n",
    "            .filter(col(\"event_name\") != \"finalize\")\n",
    "           )\n",
    "\n",
    "stupidDF.explain(True)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "c021af63-d399-4828-bca2-02566ed68c0f",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Caching\n",
    "\n",
    "By default the data of a DataFrame is present on a Spark cluster only while it is being processed during a query -- it is not automatically persisted on the cluster afterwards. (Spark is a data processing engine, not a data storage system.) You can explicity request Spark to persist a DataFrame on the cluster by invoking its `cache` method.\n",
    "\n",
    "If you do cache a DataFrame, you should always explictly evict it from cache by invoking `unpersist` when you no longer need it.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_best_32.png\" alt=\"Best Practice\"> Caching a DataFrame can be appropriate if you are certain that you will use the same DataFrame multiple times, as in:\n",
    "\n",
    "- Exploratory data analysis\n",
    "- Machine learning model training\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_warn_32.png\" alt=\"Warning\"> Aside from those use cases, you should **not** cache DataFrames because it is likely that you'll *degrade* the performance of your application.\n",
    "\n",
    "- Caching consumes cluster resources that could otherwise be used for task execution\n",
    "- Caching can prevent Spark from performing query optimizations, as shown in the next example"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "7dfa2823-1df1-457f-a70b-4a420cb98dc8",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predicate Pushdown\n",
    "\n",
    "Here is example reading from a JDBC source, where Catalyst determines that *predicate pushdown* can take place."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "23bf40e9-2511-42a0-8f64-76138ca8c1c3",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%scala\n",
    "// Ensure that the driver class is loaded\n",
    "Class.forName(\"org.postgresql.Driver\")"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "f3cfc32f-4115-498f-ac59-7f5289d8372f",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "jdbcURL = \"jdbc:postgresql://54.213.33.240/training\"\n",
    "\n",
    "# Username and Password w/read-only rights\n",
    "connProperties = {\n",
    "    \"user\" : \"training\",\n",
    "    \"password\" : \"training\"\n",
    "}\n",
    "\n",
    "ppDF = (spark\n",
    "        .read\n",
    "        .jdbc(\n",
    "            url=jdbcURL,                  # the JDBC URL\n",
    "            table=\"training.people_1m\",   # the name of the table\n",
    "            column=\"id\",                  # the name of a column of an integral type that will be used for partitioning\n",
    "            lowerBound=1,                 # the minimum value of columnName used to decide partition stride\n",
    "            upperBound=1000000,           # the maximum value of columnName used to decide partition stride\n",
    "            numPartitions=8,              # the number of partitions/connections\n",
    "            properties=connProperties     # the connection properties\n",
    "        )\n",
    "        .filter(col(\"gender\") == \"M\")   # Filter the data by gender\n",
    "       )\n",
    "\n",
    "ppDF.explain()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "99a220e3-35e9-416e-98c4-73426c76943f",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note the lack of a **Filter** and the presence of a **PushedFilters** in the **Scan**. The filter operation is pushed to the database and only the matching records are sent to Spark. This can greatly reduce the amount of data that Spark needs to ingest."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "302a6527-7799-46c0-9c27-e4693ba16418",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### No Predicate Pushdown\n",
    "\n",
    "In comparison, caching the data before filtering eliminates the possibility for the predicate push down."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "d654cce4-311a-4103-8319-0c728a3b956c",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "cachedDF = (spark\n",
    "            .read\n",
    "            .jdbc(\n",
    "                url=jdbcURL,\n",
    "                table=\"training.people_1m\",\n",
    "                column=\"id\",\n",
    "                lowerBound=1,\n",
    "                upperBound=1000000,\n",
    "                numPartitions=8,\n",
    "                properties=connProperties\n",
    "            )\n",
    "           )\n",
    "\n",
    "cachedDF.cache()\n",
    "filteredDF = cachedDF.filter(col(\"gender\") == \"M\")\n",
    "\n",
    "filteredDF.explain()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "a8b1eac9-d3bf-4afd-ac6c-cc6e7e6d23b7",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "In addition to the **Scan** (the JDBC read) we saw in the previous example, here we also see the **InMemoryTableScan** followed by a **Filter** in the explain plan.\n",
    "\n",
    "This means Spark had to read ALL the data from the database and cache it, and then scan it in cache to find the records matching the filter condition."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "0b0c8718-d927-48b8-9deb-eb68853bdef6",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remember to clean up after ourselves!"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "fef8b41f-95b0-4bf7-a689-e8ab5c4ac496",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "cachedDF.unpersist()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "9493d7db-1a26-4571-b8fd-760eb634e8aa",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clean up classroom"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "a6484785-59bb-437b-af6c-6111be26790f",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%run ./Includes/Classroom-Cleanup"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "224803bd-7478-4487-8ab4-cceb511b731a",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "notebookName": "ASP 3.2 - Query Optimization",
   "dashboards": [],
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "language": "python",
   "widgets": {},
   "notebookOrigID": 4185566132527163
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}