{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DataFrames and Transformations Review\n",
    "## De-Duping Data Lab\n",
    "\n",
    "In this exercise, we're doing ETL on a file we've received from a customer. That file contains data about people, including:\n",
    "\n",
    "* first, middle and last names\n",
    "* gender\n",
    "* birth date\n",
    "* Social Security number\n",
    "* salary\n",
    "\n",
    "But, as is unfortunately common in data we get from this customer, the file contains some duplicate records. Worse:\n",
    "\n",
    "* In some of the records, the names are mixed case (e.g., \"Carol\"), while in others, they are uppercase (e.g., \"CAROL\").\n",
    "* The Social Security numbers aren't consistent either. Some of them are hyphenated (e.g., \"992-83-4829\"), while others are missing hyphens (\"992834829\").\n",
    "\n",
    "If all of the name fields match -- if you disregard character case -- then the birth dates and salaries are guaranteed to match as well,\n",
    "and the Social Security Numbers *would* match if they were somehow put in the same format.\n",
    "\n",
    "Your job is to remove the duplicate records. The specific requirements of your job are:\n",
    "\n",
    "* Remove duplicates. It doesn't matter which record you keep; it only matters that you keep one of them.\n",
    "* Preserve the data format of the columns. For example, if you write the first name column in all lowercase, you haven't met this requirement.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_hint_32.png\" alt=\"Hint\"> The initial dataset contains 103,000 records.\n",
    "The de-duplicated result has 100,000 records.\n",
    "\n",
    "Next, write the results in **Delta** format as a **single data file** to the directory given by the variable *deltaDestDir*.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_hint_32.png\" alt=\"Hint\"> Remember the relationship between the number of partitions in a DataFrame and the number of files written.\n",
    "\n",
    "##### Methods\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#input-and-output\" target=\"_blank\">DataFrameReader</a>\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html\" target=\"_blank\">DataFrame</a>\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html?#functions\" target=\"_blank\">Built-In Functions</a>\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#input-and-output\" target=\"_blank\">DataFrameWriter</a>"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "166692d3-8ef5-44cd-b3d3-1f58fcab2853",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%run ./Includes/Classroom-Setup"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "4114afa7-3eca-4677-b98c-7d9861dc3a6a",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "It's helpful to look at the file first, so you can check the format. `dbutils.fs.head()` (or just `%fs head`) is a big help here."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "8beeec4e-0ee9-4faa-adae-ebc322fe8fdf",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%fs head dbfs:/mnt/training/dataframes/people-with-dups.txt"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "c70acfab-d316-4ebf-a678-6bfaff7c4b62",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO\n",
    "\n",
    "sourceFile = \"dbfs:/mnt/training/dataframes/people-with-dups.txt\"\n",
    "destFile = workingDir + \"/people.parquet\"\n",
    "\n",
    "# In case it already exists\n",
    "dbutils.fs.rm(destFile, True)\n",
    "\n",
    "# Complete your work here...\n"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "13d7cd0c-9b9f-45de-8a80-8c20f4683c8d",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "**CHECK YOUR WORK**"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "600bc335-3985-4267-8bcb-48a7588a5bd4",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "verify_files = dbutils.fs.ls(deltaDestDir)\n",
    "verify_delta_format = False\n",
    "verify_num_data_files = 0\n",
    "for f in verify_files:\n",
    "    if f.name == '_delta_log/':\n",
    "        verify_delta_format = True\n",
    "    elif f.name.endswith('.parquet'):\n",
    "        verify_num_data_files += 1\n",
    "\n",
    "assert verify_delta_format, \"Data not written in Delta format\"\n",
    "assert verify_num_data_files == 1, \"Expected 1 data file written\"\n",
    "\n",
    "verify_record_count = spark.read.format(\"delta\").load(deltaDestDir).count()\n",
    "assert verify_record_count == 100000, \"Expected 100000 records in final result\"\n",
    "\n",
    "del verify_files, verify_delta_format, verify_num_data_files, verify_record_count"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "4ec6bedc-b959-45ef-ad52-acce3765f6a6",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clean up classroom\n",
    "Run the cell below to clean up resources."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "afad540f-da94-4bc7-8313-3a2d6838556e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%run \"./Includes/Classroom-Cleanup\""
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "1777597e-9116-479a-a14d-a6d8fecffa84",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "notebookName": "ASP 3.4 - Review",
   "dashboards": [],
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "language": "python",
   "widgets": {},
   "notebookOrigID": 4185566132528415
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}