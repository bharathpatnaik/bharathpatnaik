{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "##### Objectives\n",
    "1. Read from CSV files\n",
    "1. Read from JSON files\n",
    "1. Write DataFrame to files\n",
    "1. Write DataFrame to tables\n",
    "1. Write DataFrame to a Delta table\n",
    "\n",
    "##### Methods\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#input-and-output\" target=\"_blank\">DataFrameReader</a>: `csv`, `json`, `option`, `schema`\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#input-and-output\" target=\"_blank\">DataFrameWriter</a>: `mode`, `option`, `parquet`, `format`, `saveAsTable`\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.types.StructType.html#pyspark.sql.types.StructType\" target=\"_blank\">StructType</a>: `toDDL`\n",
    "\n",
    "##### Spark Types\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#data-types\" target=\"_blank\">Types</a>: `ArrayType`, `DoubleType`, `IntegerType`, `LongType`, `StringType`, `StructType`, `StructField`"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "2f5270fe-3a17-43ce-8f19-4c646c0f81d5",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%run ./Includes/Classroom-Setup"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "679431bc-1721-483a-9a3a-f1262f42c04c",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DataFrameReader\n",
    "Interface used to load a DataFrame from external storage systems\n",
    "\n",
    "```\n",
    "spark.read.parquet(\"path/to/files\")\n",
    "```\n",
    "\n",
    "DataFrameReader is accessible through the SparkSession attribute `read`. This class includes methods to load DataFrames from different external storage systems."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "6b80a67b-f6a4-4b1d-8293-04ad41425b94",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read from CSV files\n",
    "Read from CSV with the DataFrameReader's `csv` method and the following options:\n",
    "\n",
    "Tab separator, use first line as header, infer schema"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "27a0bf06-f52d-4c32-812f-9cf3cfd27def",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "usersCsvPath = \"/mnt/training/ecommerce/users/users-500k.csv\"\n",
    "\n",
    "usersDF = (spark\n",
    "           .read\n",
    "           .option(\"sep\", \"\\t\")\n",
    "           .option(\"header\", True)\n",
    "           .option(\"inferSchema\", True)\n",
    "           .csv(usersCsvPath)\n",
    "          )\n",
    "\n",
    "usersDF.printSchema()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "7d7c2805-0e09-4558-8c17-609a34a12166",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Spark's Python API also allows you to specify the DataFrameReader options as parameters to the `csv` method"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "e53f1add-b609-40fc-a427-3c5c48a1148d",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "usersDF = (spark\n",
    "           .read\n",
    "           .csv(usersCsvPath, sep=\"\\t\", header=True, inferSchema=True)\n",
    "          )\n",
    "\n",
    "usersDF.printSchema()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "0b6ed3f1-8945-4fbc-86e4-887f46d9a196",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Manually define the schema by creating a `StructType` with column names and data types"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "3c220657-0789-4a8a-95dd-a0534627d772",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql.types import LongType, StringType, StructType, StructField\n",
    "\n",
    "userDefinedSchema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"user_first_touch_timestamp\", LongType(), True),\n",
    "    StructField(\"email\", StringType(), True)\n",
    "])"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "ddddcd65-6834-40cc-b7ea-7fa4f7eb97d2",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read from CSV using this user-defined schema instead of inferring the schema"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "4692f5c4-9c9e-41bb-8c6e-a45a96114001",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "usersDF = (spark\n",
    "           .read\n",
    "           .option(\"sep\", \"\\t\")\n",
    "           .option(\"header\", True)\n",
    "           .schema(userDefinedSchema)\n",
    "           .csv(usersCsvPath)\n",
    "          )"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "0c178102-5a1f-4951-aa40-e92e418ce2ad",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Alternatively, define the schema using <a href=\"https://en.wikipedia.org/wiki/Data_definition_language\" target=\"_blank\">data definition language (DDL)</a> syntax."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "6a4fc14f-48e4-4522-8089-2f1019cfba79",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "DDLSchema = \"user_id string, user_first_touch_timestamp long, email string\"\n",
    "\n",
    "usersDF = (spark\n",
    "           .read\n",
    "           .option(\"sep\", \"\\t\")\n",
    "           .option(\"header\", True)\n",
    "           .schema(DDLSchema)\n",
    "           .csv(usersCsvPath)\n",
    "          )"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "9480bb73-39ce-4a1b-a1a5-6e35d05b08f7",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read from JSON files\n",
    "\n",
    "Read from JSON with DataFrameReader's `json` method and the infer schema option"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "010057d9-6a6b-4dbc-af2b-4423cf375d31",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "eventsJsonPath = \"/mnt/training/ecommerce/events/events-500k.json\"\n",
    "\n",
    "eventsDF = (spark\n",
    "            .read\n",
    "            .option(\"inferSchema\", True)\n",
    "            .json(eventsJsonPath)\n",
    "           )\n",
    "\n",
    "eventsDF.printSchema()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "24f75fb6-2713-4f38-93d5-9d58b5baba0c",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read data faster by creating a `StructType` with the schema names and data types"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "8cd12502-592a-4e51-acce-6e5e6116e5c4",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql.types import ArrayType, DoubleType, IntegerType, LongType, StringType, StructType, StructField\n",
    "\n",
    "userDefinedSchema = StructType([\n",
    "    StructField(\"device\", StringType(), True),\n",
    "    StructField(\"ecommerce\", StructType([\n",
    "        StructField(\"purchaseRevenue\", DoubleType(), True),\n",
    "        StructField(\"total_item_quantity\", LongType(), True),\n",
    "        StructField(\"unique_items\", LongType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"event_name\", StringType(), True),\n",
    "    StructField(\"event_previous_timestamp\", LongType(), True),\n",
    "    StructField(\"event_timestamp\", LongType(), True),\n",
    "    StructField(\"geo\", StructType([\n",
    "        StructField(\"city\", StringType(), True),\n",
    "        StructField(\"state\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"items\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"coupon\", StringType(), True),\n",
    "            StructField(\"item_id\", StringType(), True),\n",
    "            StructField(\"item_name\", StringType(), True),\n",
    "            StructField(\"item_revenue_in_usd\", DoubleType(), True),\n",
    "            StructField(\"price_in_usd\", DoubleType(), True),\n",
    "            StructField(\"quantity\", LongType(), True)\n",
    "        ])\n",
    "    ), True),\n",
    "    StructField(\"traffic_source\", StringType(), True),\n",
    "    StructField(\"user_first_touch_timestamp\", LongType(), True),\n",
    "    StructField(\"user_id\", StringType(), True)\n",
    "])\n",
    "\n",
    "eventsDF = (spark\n",
    "            .read\n",
    "            .schema(userDefinedSchema)\n",
    "            .json(eventsJsonPath)\n",
    "           )"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "8f6e6af1-ed54-4646-bda1-ea1d352bfa31",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can use the `StructType` Scala method `toDDL` to have a DDL-formatted string created for you.\n",
    "\n",
    "In a Python notebook, create a Scala cell to create the string to copy and paste."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "d7c185d5-53f7-4b7f-9667-6541aacec959",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%scala\n",
    "spark.read.parquet(\"/mnt/training/ecommerce/events/events.parquet\").schema.toDDL"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "578ccad1-1725-40fa-9363-bc24eea7551b",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "DDLSchema = \"`device` STRING,`ecommerce` STRUCT<`purchase_revenue_in_usd`: DOUBLE, `total_item_quantity`: BIGINT, `unique_items`: BIGINT>,`event_name` STRING,`event_previous_timestamp` BIGINT,`event_timestamp` BIGINT,`geo` STRUCT<`city`: STRING, `state`: STRING>,`items` ARRAY<STRUCT<`coupon`: STRING, `item_id`: STRING, `item_name`: STRING, `item_revenue_in_usd`: DOUBLE, `price_in_usd`: DOUBLE, `quantity`: BIGINT>>,`traffic_source` STRING,`user_first_touch_timestamp` BIGINT,`user_id` STRING\"\n",
    "\n",
    "eventsDF = (spark\n",
    "            .read\n",
    "            .schema(DDLSchema)\n",
    "            .json(eventsJsonPath)\n",
    "           )"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "9eaaa7b9-133e-42bc-9eec-1fffab91e897",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DataFrameWriter\n",
    "Interface used to write a DataFrame to external storage systems\n",
    "\n",
    "```\n",
    "(df.write                         \n",
    "  .option(\"compression\", \"snappy\")\n",
    "  .mode(\"overwrite\")      \n",
    "  .parquet(outPath)       \n",
    ")\n",
    "```\n",
    "\n",
    "DataFrameWriter is accessible through the SparkSession attribute `write`. This class includes methods to write DataFrames to different external storage systems."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "723e02a2-48e0-453a-8d61-0f415efb9e58",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Write DataFrames to files\n",
    "\n",
    "Write `usersDF` to parquet with DataFrameWriter's `parquet` method and the following configurations:\n",
    "\n",
    "Snappy compression, overwrite mode"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "2206712d-ec0b-4d75-aaf4-2ee91b8e65d4",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "usersOutputPath = workingDir + \"/users.parquet\"\n",
    "\n",
    "(usersDF\n",
    " .write\n",
    " .option(\"compression\", \"snappy\")\n",
    " .mode(\"overwrite\")\n",
    " .parquet(usersOutputPath)\n",
    ")"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "c97bd187-5cfc-4928-9390-84ca62e07529",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "display(\n",
    "    dbutils.fs.ls(usersOutputPath)\n",
    ")"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "513657f0-7a1e-4d91-ab65-eb5800cf3ab1",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "As with DataFrameReader, Spark's Python API also allows you to specify the DataFrameWriter options as parameters to the `parquet` method"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "06a67ff8-f6d8-43fb-b90f-77fa589a2d6e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "(usersDF\n",
    " .write\n",
    " .parquet(usersOutputPath, compression=\"snappy\", mode=\"overwrite\")\n",
    ")"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "27d1af41-d10c-4978-8777-0eb575a17f7e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Write DataFrames to tables\n",
    "\n",
    "Write `eventsDF` to a table using the DataFrameWriter method `saveAsTable`\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> This creates a global table, unlike the local view created by the DataFrame method `createOrReplaceTempView`"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "405f864d-b10c-4fa9-bbbc-204aa0d0a1c0",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "eventsDF.write.mode(\"overwrite\").saveAsTable(\"events_p\")"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "4b9a6c1b-89b6-446a-a452-3b4f44afee60",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "This table was saved in the database created for you in classroom setup. See database name printed below."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "81bf9109-af1b-42ca-8b92-6f6883f6ae85",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(databaseName)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "4df70cea-3463-491c-8fb9-a4fa43ce8a85",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Delta Lake\n",
    "\n",
    "In almost all cases, the best practice is to use Delta Lake format, especially whenever the data will be referenced from a Databricks workspace. \n",
    "\n",
    "<a href=\"https://delta.io/\" target=\"_blank\">Delta Lake</a> is an open source technology designed to work with Spark to bring reliability to data lakes.\n",
    "\n",
    "![delta](https://files.training.databricks.com/images/aspwd/delta_storage_layer.png)\n",
    "\n",
    "#### Delta Lake's Key Features\n",
    "- ACID transactions\n",
    "- Scalable metadata handline\n",
    "- Unified streaming and batch processing\n",
    "- Time travel (data versioning)\n",
    "- Schema enforcement and evolution\n",
    "- Audit history\n",
    "- Parquet format\n",
    "- Compatible with Apache Spark API"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "f8bc6b06-8d55-42dd-9f1b-e8063bf8102a",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Write Results to a Delta Table\n",
    "\n",
    "Write `eventsDF` with the DataFrameWriter's `save` method and the following configurations: Delta format, overwrite mode"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "737bac08-89f6-4f81-b8a2-c51a779d843a",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "eventsOutputPath = workingDir + \"/delta/events\"\n",
    "\n",
    "(eventsDF\n",
    " .write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .save(eventsOutputPath)\n",
    ")"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "dd6ed12d-5dff-4c44-893b-c52568fd2fc9",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ingesting Data Lab\n",
    "\n",
    "Read in CSV files containing products data.\n",
    "\n",
    "##### Tasks\n",
    "1. Read with infer schema\n",
    "2. Read with user-defined schema\n",
    "3. Read with schema as DDL formatted string\n",
    "4. Write using Delta format"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "9f9c31f4-5bd3-4ef7-ad78-203c3acd873e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Read with infer schema\n",
    "- View the first CSV file using DBUtils method `fs.head` with the filepath provided in the variable `singleProductCsvFilePath`\n",
    "- Create `productsDF` by reading from CSV files located in the filepath provided in the variable `productsCsvPath`\n",
    "  - Configure options to use first line as header and infer schema"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "288e6c7e-1ad1-4db5-89d0-cfb06040c87c",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO\n",
    "singleProductCsvFilePath = \"/mnt/training/ecommerce/products/products.csv/part-00000-tid-1663954264736839188-daf30e86-5967-4173-b9ae-d1481d3506db-2367-1-c000.csv\"\n",
    "\n",
    "print(FILL_IN)\n",
    "\n",
    "productsCsvPath = \"/mnt/training/ecommerce/products/products.csv\"\n",
    "\n",
    "productsDF = FILL_IN\n",
    "\n",
    "productsDF.printSchema()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "387388e5-68ef-492b-8d08-516508b0c1e3",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "**CHECK YOUR WORK**"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "2a13d3d6-7431-400d-b359-9b74a3fd6602",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "assert(productsDF.count() == 12)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "1293d2e7-26a1-4e89-956a-d7b4a671da45",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Read with user-defined schema\n",
    "Define schema by creating a `StructType` with column names and data types"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "9aea18b8-3255-44cb-b32a-117949605eab",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO\n",
    "userDefinedSchema = FILL_IN\n",
    "\n",
    "productsDF2 = FILL_IN"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "31d02549-fc14-4114-a5eb-1fb290d23e3c",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "**CHECK YOUR WORK**"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "3172b93d-14cc-4a5e-9e95-3e6b6fb0d43d",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "assert(userDefinedSchema.fieldNames() == [\"item_id\", \"name\", \"price\"])"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "3b2ca039-f628-4066-beec-42a2b55fc3b1",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "expected1 = Row(item_id=\"M_STAN_Q\", name=\"Standard Queen Mattress\", price=1045.0)\n",
    "result1 = productsDF2.first()\n",
    "\n",
    "assert(expected1 == result1)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "b18c4ddc-a81b-4e13-b063-090eaa70ee20",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Read with DDL formatted string"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "10c8ed42-b64f-4b33-9e3f-f04a8158f9e7",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO\n",
    "DDLSchema = FILL_IN\n",
    "\n",
    "productsDF3 = FILL_IN"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "07ce31da-508f-4b62-88fa-a7870a61150c",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "**CHECK YOUR WORK**"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "c80ff38d-1f84-4acf-8f4e-2ebc015ad0ac",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "assert(productsDF3.count() == 12)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "ce80b442-0571-4836-9f73-156ec890d1f7",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Write to Delta\n",
    "Write `productsDF` to the filepath provided in the variable `productsOutputPath`"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "5420eacb-db65-46f0-ab10-6e1355e61ee2",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO\n",
    "productsOutputPath = workingDir + \"/delta/products\"\n",
    "productsDF.FILL_IN"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "ea603b15-8b94-412d-ad71-ba8c8acbda24",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "**CHECK YOUR WORK**"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "1fb1c2a1-e6f1-4dc2-8f28-8fc2dcbc4ccd",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "verify_files = dbutils.fs.ls(productsOutputPath)\n",
    "verify_delta_format = False\n",
    "verify_num_data_files = 0\n",
    "for f in verify_files:\n",
    "    if f.name == '_delta_log/':\n",
    "        verify_delta_format = True\n",
    "    elif f.name.endswith('.parquet'):\n",
    "        verify_num_data_files += 1\n",
    "\n",
    "assert verify_delta_format, \"Data not written in Delta format\"\n",
    "assert verify_num_data_files > 0, \"No data written\"\n",
    "del verify_files, verify_delta_format, verify_num_data_files"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "5d5c0db8-46fb-4ac0-90b1-64e565eb94f1",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clean up classroom"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "49f9c77d-751e-4a3c-bb32-b6fb1d696aca",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%run ./Includes/Classroom-Cleanup"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "77d455d6-fb9d-4b5e-9def-fe21eb4198cb",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "notebookName": "ASP 1.4 - Reader & Writer",
   "dashboards": [],
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "language": "python",
   "widgets": {},
   "notebookOrigID": 4185566132527522
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}