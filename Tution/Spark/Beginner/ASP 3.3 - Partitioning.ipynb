{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Partitioning\n",
    "##### Objectives\n",
    "1. Get partitions and cores\n",
    "1. Repartition DataFrames\n",
    "1. Configure default shuffle partitions\n",
    "\n",
    "##### Methods\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html\" target=\"_blank\">DataFrame</a>: `repartition`, `coalesce`, `rdd.getNumPartitions`\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkConf.html?#pyspark.SparkConf\" target=\"_blank\">SparkConf</a>: `get`, `set`\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#spark-session-apis\" target=\"_blank\">SparkSession</a>: `spark.sparkContext.defaultParallelism`\n",
    "\n",
    "##### SparkConf Parameters\n",
    "- `spark.sql.shuffle.partitions`, `spark.sql.adaptive.enabled`"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "111e3052-c864-4341-a17c-89d769f42016",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%run ./Includes/Classroom-Setup"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "f5ae58b5-c647-471d-8b17-83e13d0cf52a",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get partitions and cores\n",
    "\n",
    "Use the `rdd` method `getNumPartitions` to get the number of DataFrame partitions."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "225aa647-147b-4694-9276-7051af48d616",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df = spark.read.parquet(eventsPath)\n",
    "df.rdd.getNumPartitions()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "ec3af7af-b40f-4f95-801a-6e26cdf7becf",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Access `SparkContext` through `SparkSession` to get the number of cores or slots.\n",
    "\n",
    "Use the `defaultParallelism` attribute to get the number of cores in a cluster."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "7107cd91-0133-4c36-8c43-85f0f479b07c",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(spark.sparkContext.defaultParallelism)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "d0a64f6c-dba0-498e-a5c3-0b24ca4ec9b7",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "`SparkContext` is also provided in Databricks notebooks as the variable `sc`."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "3d15e27d-5835-40f7-b665-401871ba47c5",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(sc.defaultParallelism)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "91fc4030-38cd-4ccb-8dab-ff8237a7a41d",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Repartition DataFrame\n",
    "\n",
    "There are two methods available to repartition a DataFrame: `repartition` and `coalesce`."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "836e005a-4da1-4669-ad03-d6db21c90108",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### `repartition`\n",
    "Returns a new DataFrame that has exactly `n` partitions.\n",
    "\n",
    "- Wide transformation\n",
    "- Pro: Evenly balances partition sizes  \n",
    "- Con: Requires shuffling all data"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "204b4392-ef4c-43d0-a48d-295bdd529a1b",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "repartitionedDF = df.repartition(8)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "d60a8bb0-5b1b-475c-a4bf-c90d1bcd4f2a",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "repartitionedDF.rdd.getNumPartitions()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "1585df3c-067c-4de3-8407-e98d360ce71e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### `coalesce`\n",
    "Returns a new DataFrame that has exactly `n` partitions, when fewer partitions are requested.\n",
    "\n",
    "If a larger number of partitions is requested, it will stay at the current number of partitions.\n",
    "\n",
    "- Narrow transformation, some partitions are effectively concatenated\n",
    "- Pro: Requires no shuffling\n",
    "- Cons:\n",
    "  - Is not able to increase # partitions\n",
    "  - Can result in uneven partition sizes"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "57b4de2f-35dd-49c2-a4bc-a41c23e2f8b9",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "coalesceDF = df.coalesce(8)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "1c2686f8-cfcf-42ca-bf09-e9a9e1901a88",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "coalesceDF.rdd.getNumPartitions()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "f34a8f18-d380-4e36-b8bb-04687b61b4f7",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Configure default shuffle partitions\n",
    "\n",
    "Use the SparkSession's `conf` attribute to get and set dynamic Spark configuration properties. The `spark.sql.shuffle.partitions` property determines the number of partitions that result from a shuffle. Let's check its default value:"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "9e37a7bb-3cbe-4ac0-8b82-a68b23df9ec3",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "146a0191-6439-4c44-9efa-007da9d27aba",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Assuming that the data set isn't too large, you could configure the default number of shuffle partitions to match the number of cores:"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "a0377dca-5d23-42d3-9655-47e059767506",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)\n",
    "print(spark.conf.get(\"spark.sql.shuffle.partitions\"))"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "017d0e65-aff0-4fd5-b7a4-1ccaa607f1e5",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Partitioning Guidelines\n",
    "- Make the number of partitions a multiple of the number of cores\n",
    "- Target a partition size of ~200MB\n",
    "- Size default shuffle partitions by dividing largest shuffle stage input by the target partition size (e.g., 4TB / 200MB = 20,000 shuffle partition count)\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> When writing a DataFrame to storage, the number of DataFrame partitions determines the number of data files written. (This assumes that <a href=\"https://sparkbyexamples.com/apache-hive/hive-partitions-explained-with-examples/\" target=\"_blank\">Hive partitioning</a> is not used for the data in storage. A discussion of DataFrame partitioning vs Hive partitioning is beyond the scope of this class.)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "fd04e1c4-dd83-4a44-a0c1-5cc5d748f928",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adaptive Query Execution\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/aspwd/partitioning_aqe.png\" width=\"60%\" />\n",
    "\n",
    "In Spark 3, <a href=\"https://spark.apache.org/docs/latest/sql-performance-tuning.html#adaptive-query-execution\" target=\"_blank\">AQE</a> is now able to <a href=\"https://databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html\" target=\"_blank\"> dynamically coalesce shuffle partitions</a> at runtime. This means that you can set `spark.sql.shuffle.partitions` based on the largest data set your application processes and allow AQE to reduce the number of partitions automatically when there is less data to process.\n",
    "\n",
    "The `spark.sql.adaptive.enabled` configuration option controls whether AQE is turned on/off."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "43c7f29c-ca70-432a-a395-a922f12321b5",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "spark.conf.get(\"spark.sql.adaptive.enabled\")"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "f7cd9b6a-96e9-49d9-9180-56ea8ceed1cc",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clean up classroom"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "0d73db60-54c6-40d9-9c4a-266313634ae1",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%run ./Includes/Classroom-Cleanup"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "0f6d87b6-1097-477a-a6e4-316c8151d26e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "notebookName": "ASP 3.3 - Partitioning",
   "dashboards": [],
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "language": "python",
   "widgets": {},
   "notebookOrigID": 4185566132527408
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}