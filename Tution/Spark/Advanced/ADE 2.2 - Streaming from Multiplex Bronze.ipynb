{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Streaming from Multiplex Bronze\n",
    "\n",
    "In this notebook, you will configure a query to consume and parse raw data from a single topic as it lands in the multiplex bronze table configured in the last lesson. We'll continue refining this query in the following notebooks.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lesson, you should be able to:\n",
    "- Describe how filters are applied to streaming jobs\n",
    "- Use built-in functions to flatten nested JSON data\n",
    "- Parse and save binary-encoded strings to native types"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "13f399c3-312d-4afd-a425-64be38b7c30e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Declare database and set all path variables."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "9c76984b-90d2-4184-8836-13ba413b6c89",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%run ../Includes/Classroom-Setup-3.2"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "cac345c0-574a-4007-9df9-3b73bc6ee809",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define a Batch Read\n",
    "\n",
    "Before building our streams, we'll start with a static view of our data. Working with static data can be easier during interactive development as no streams will be triggered. \n",
    "\n",
    "Because we're working with Delta Lake as our source, we'll still get the most up-to-date version of our table each time we execute a query.\n",
    "\n",
    "If you're working with SQL, you can just directly query the **`bronze`** table registered in the previous lesson. \n",
    "\n",
    "Python and Scala users can easily create a Dataframe from a registered table."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "6ddc832b-7515-4c5e-979e-e99acbfd0cc0",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "batch_df = spark.table(\"bronze\")\n",
    "display(batch_df)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "df003ed9-c0b1-4867-9b2d-1f9b92ea1ccf",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Delta Lake stores our schema information. \n",
    "\n",
    "Let's print it out, just to recall it's structure:"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "2d013f62-740e-42f8-957c-c9cd8f71a24f",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "DESCRIBE bronze"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "7e079137-49a4-4c87-8257-5bd3ba08c0ba",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preview your data."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "db6a1a0e-7c7a-412b-ac98-74d48125720e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM bronze\n",
    "LIMIT 20"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "532f2d94-1f94-4080-bc36-5b3441963c36",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are multiple topics being ingested. So, we'll need to define logic for each of these topics separately."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "3507ec12-8dd3-468a-b08a-799db23c9702",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT DISTINCT(topic)\n",
    "FROM bronze"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "9cae669d-96c9-485f-8412-b6c0530777b1",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll cast our binary fields as strings, as this will allow us to manually review their contents."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "43ee3f93-eff8-4b9c-9e90-0ed0fb252003",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT cast(key AS STRING), cast(value AS STRING)\n",
    "FROM bronze\n",
    "LIMIT 20"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "ba513d47-a885-453f-8f78-e79a4f952fa4",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parse Heart Rate Recordings\n",
    "\n",
    "Let's start by defining logic to parse our heart rate recordings. We'll write this logic against our static data. Note that there are some <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#unsupported-operations\" target=\"_blank\">unsupported operations</a> in Structured Streaming, so we may need to refactor some of our logic if we don't build our current queries with these limitations in mind.\n",
    "\n",
    "Together, we'll iteratively develop a single query that parses our **`bpm`** topic to the following schema.\n",
    "\n",
    "| field | type |\n",
    "| --- | --- |\n",
    "| device_id | LONG | \n",
    "| time | TIMESTAMP | \n",
    "| heartrate | DOUBLE |\n",
    "\n",
    "We'll be creating the table **`heartrate_silver`** in our architectural diagram.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/ade/ADE_arch_heartrate_silver.png\" width=\"60%\" />"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "31ed52be-5371-4b0f-8024-cbd26f2141e2",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT v.*\n",
    "FROM (\n",
    "  SELECT from_json(cast(value AS STRING), \"device_id LONG, time TIMESTAMP, heartrate DOUBLE\") v\n",
    "  FROM bronze\n",
    "  WHERE topic = \"bpm\")"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "aab6308e-7620-4d96-a031-41b96bfb0605",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convert Logic for Streaming Read\n",
    "\n",
    "We can define a streaming read directly against our Delta table. Note that most configuration for streaming queries is done on write rather than read, so here we see little change to our above logic.\n",
    "\n",
    "The cell below shows how to convert a static table into a streaming temp view (if you wish to write streaming queries with Spark SQL)."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "5147c4ce-1b16-4be4-a0b6-73840d9a4e2a",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "(spark.readStream\n",
    "      .table(\"bronze\")\n",
    "      .createOrReplaceTempView(\"TEMP_bronze\"))"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "d40ac6d4-13e7-4879-ad2b-aadcb9984632",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Updating our above query to refer to this temp view gives us a streaming result."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "8be5c3da-3a58-424c-84fb-e3f50127450d",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT v.*\n",
    "FROM (\n",
    "  SELECT from_json(cast(value AS STRING), \"device_id LONG, time TIMESTAMP, heartrate DOUBLE\") v\n",
    "  FROM TEMP_bronze\n",
    "  WHERE topic = \"bpm\")"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "b2d16ddf-5bd4-4d4f-94b7-7c5ccb3bfba7",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that anytime a streaming read is displayed to a notebook, a streaming job will begin and if allowed to run forever this will prevent the cluster from auto-terminating.  You can stop the stream clicking the \"Cancel\" link in the cell above, clicking \"Stop Execution\" at the top of the notebook, or running the code below.\n",
    "\n",
    "Stop the streaming display above before continuing."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "0b513dab-2e74-41b5-b819-780ece507f8a",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for stream in spark.streams.active:\n",
    "    stream.stop()\n",
    "    stream.awaitTermination()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "5a93d8fb-4ee8-4946-97ce-a3aaccf6184d",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "To persist results to disk, a streaming write will need to be performed using Python.  We can switch from SQL to Python by using a temporary view as an intermediary to capture the query we want to apply."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "f1a50e34-3373-462c-90f5-d39f51a1badf",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW TEMP_SILVER AS\n",
    "  SELECT v.*\n",
    "  FROM (\n",
    "    SELECT from_json(cast(value AS STRING), \"device_id LONG, time TIMESTAMP, heartrate DOUBLE\") v\n",
    "    FROM TEMP_bronze\n",
    "    WHERE topic = \"bpm\")"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "cd97367a-714a-4d74-8b02-e39769f0a11e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read from the streaming **`TEMP_SILVER`** temporary view and write to the **`heart_rate_silver`** delta table.\n",
    "\n",
    "Using the **`trigger(availableNow=True)`** option will process all records (in multiple batches if needed) until no more data is available and then stop the stream."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "f84fa2c7-b669-414a-ae62-52b4afc69a04",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "query = (spark.table(\"TEMP_SILVER\").writeStream\n",
    "               .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/heart_rate\")\n",
    "               .option(\"path\", f\"{DA.paths.user_db}/heart_rate_silver.delta\")\n",
    "               .trigger(availableNow=True)\n",
    "               .table(\"heart_rate_silver\"))\n",
    "\n",
    "query.awaitTermination()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "f5cbc8a2-7790-4392-ad3b-343c7e3e1e37",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Alternatively, instead of using SQL, the entire job can be expressed using Python Dataframes API.  The cell below has this logic refactored to Python."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "3744f6c1-565d-4765-9d6d-6132135cb462",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "json_schema = \"device_id LONG, time TIMESTAMP, heartrate DOUBLE\"\n",
    "\n",
    "(spark\n",
    "   .readStream.table(\"bronze\")\n",
    "   .filter(\"topic = 'bpm'\")\n",
    "   .select(F.from_json(F.col(\"value\").cast(\"string\"), json_schema).alias(\"v\"))\n",
    "   .select(\"v.*\")\n",
    "   .writeStream\n",
    "       .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/heart_rate\")\n",
    "       .option(\"path\", f\"{DA.paths.user_db}/heart_rate_silver.delta\")\n",
    "       .trigger(availableNow=True)\n",
    "       .table(\"heart_rate_silver\"))\n",
    "\n",
    "query.awaitTermination()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "fa5e7b21-e6d4-45a5-ac0b-a6caf47db456",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"https://files.training.databricks.com/images/icon_warn_32.png\"> Before continuing, make sure you cancel any streams. The **`Run All`** button at the top of the screen will say **`Stop Execution`** if you have a stream still running.  Or run the code below to stop all streaming currently running on this cluster."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "fdb9bf81-448d-4ec6-a67d-f142d51bbded",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for stream in spark.streams.active:\n",
    "    stream.stop()\n",
    "    stream.awaitTermination()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "a3046914-da92-4794-8f75-bfeafb6ecdc1",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Silver Table Motivations\n",
    "\n",
    "In addition to parsing records and flattening and changing our schema, we should also check the quality of our data before writing to our silver tables.\n",
    "\n",
    "In the following notebooks, we'll review various quality checks."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "23a26760-092d-4fa2-aa3d-698656f674f0",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the following cell to delete the tables and files associated with this lesson."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "3dd3ac9b-a7b8-470d-9ddf-b7bb68ac3fed",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "DA.cleanup()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "04cca4f7-c824-40e7-90c8-c1121be8a780",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "notebookName": "ADE 2.2 - Streaming from Multiplex Bronze",
   "dashboards": [],
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "language": "python",
   "widgets": {},
   "notebookOrigID": 4185566132530553
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}