{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Processing Records from Delta Change Data Feed\n",
    "\n",
    "In this notebook, we'll demonstrate an end-to-end of how you can easily propagate changes through a Lakehouse with Delta Lake Change Data Feed (CDF).\n",
    "\n",
    "For this demo, we'll work with a slightly different dataset representing patient information for medical records. Descriptions of the data at various stages follow.\n",
    "\n",
    "### Bronze Table\n",
    "Here we store all records as consumed. A row represents:\n",
    "1. A new patient providing data for the first time\n",
    "1. An existing patient confirming that their information is still correct\n",
    "1. An existing patient updating some of their information\n",
    "\n",
    "The type of action a row represents is not captured.\n",
    "\n",
    "### Silver Table\n",
    "This is the validated view of our data. Each patient will appear only once in this table. An upsert statement will be used to identify rows that have changed.\n",
    "\n",
    "### Gold Table\n",
    "For this example, we'll create a simple gold table that captures patients that have a new address.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lesson, students will be able to:\n",
    "- Enable Change Data Feed on a cluster or for a particular table\n",
    "- Describe how changes are recorded\n",
    "- Read CDF output with Spark SQL or PySpark\n",
    "- Refactor ELT code to process CDF output"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "0c0c6dc9-f541-4158-bd43-4c7e07a82645",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup\n",
    "\n",
    "The following code defines some paths, a demo database, and clears out previous runs of the demo.\n",
    "\n",
    "It also defines another data factory that we'll use to land raw data in our source directory, allowing us to process new records as if they were arriving in production."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "bd4148df-2d2b-416d-8234-1e9b912e609f",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%run ../Includes/Classroom-Setup-7.1"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "ccb36713-6a53-473a-a08c-0d8ef006e814",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Enable CDF using Spark conf setting in a notebook or on a cluster will ensure it's used on all newly created Delta tables in that scope."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "622ea1d0-18e3-4545-9f85-9d35c7853d39",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "spark.conf.set(\"spark.databricks.delta.properties.defaults.enableChangeDataFeed\", True)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "5f0ced75-d19f-4d0d-8b73-0ef20292b063",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ingest Data with Auto Loader\n",
    "\n",
    "Here we'll use Auto Loader to ingest data as it arrives.\n",
    "\n",
    "The steps below include:\n",
    "* Declaring the target table\n",
    "* Creating & starting the stream\n",
    "* Load some data into our source directory"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "f7652b2a-7858-42bc-8add-8bbaf0675539",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create the bronze table."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "45dc62b9-ddbc-4fd6-875c-c12e57f3c1d6",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS bronze\n",
    "  (mrn BIGINT, dob DATE, sex STRING, gender STRING, first_name STRING, last_name STRING, street_address STRING, zip BIGINT, city STRING, state STRING, updated timestamp) \n",
    "LOCATION '${da.paths.working_dir}/bronze'"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "b9d2925f-8bda-4707-9fda-2f8d2acdd403",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create and start the stream.\n",
    "\n",
    "For this example, we will:\n",
    "* Use continuous processing as opposed to trigger-once or trigger-available-now\n",
    "* Specify the schema as opposed to inferring it"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "70456395-0f49-4ee8-8995-9b623f04bd66",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "schema = \"mrn BIGINT, dob DATE, sex STRING, gender STRING, first_name STRING, last_name STRING, street_address STRING, zip BIGINT, city STRING, state STRING, updated TIMESTAMP\"\n",
    "\n",
    "bronze_query = (spark.readStream\n",
    "                     .format(\"cloudFiles\")\n",
    "                     .option(\"cloudFiles.format\", \"json\")\n",
    "                     .schema(schema)\n",
    "                     .load(DA.paths.cdc_stream)\n",
    "                     .writeStream\n",
    "                     .format(\"delta\")\n",
    "                     .outputMode(\"append\")\n",
    "                     #.trigger(availableNow=True)\n",
    "                     .trigger(processingTime='5 seconds')\n",
    "                     .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/bronze\")\n",
    "                     .table(\"bronze\"))\n",
    "\n",
    "DA.block_until_stream_is_ready(bronze_query)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "e7309df2-c41b-4729-8b77-844b2ac7ea57",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Expand the stream monitor above to see the progress of your stream. \n",
    "\n",
    "No files should have been ingested.\n",
    "\n",
    "Use the cell below to land a batch of data; you should see these records processed as a single batch."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "509d57cc-d85a-4e45-9bd0-95050b2406ba",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "DA.cdc_stream.load()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "aaf7f8f1-9d25-4dfb-95ef-aa5066461334",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a Target Table\n",
    "\n",
    "Here we use **`DEEP CLONE`** to move read-only data from PROD to our DEV environment (where we have full write/delete access)."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "4d0e927d-aaf2-492f-9aaf-e476de6f1b08",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "CREATE TABLE silver\n",
    "DEEP CLONE delta.`${da.paths.silver_source}`\n",
    "LOCATION '${da.paths.user_db}/silver'"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "4d0ff348-ce1c-4421-859d-0c920156a0fc",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tables that were not created with CDF enabled will not have it turned on by default, but can be altered to capture changes with the following syntax.\n",
    "\n",
    "Note that editing properties will version a table. \n",
    "\n",
    "Note: CDC data is **NOT** captured during **`CLONE`** operations."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "b2fc802c-80fc-47f7-8973-5ec3b1738928",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "ALTER TABLE silver \n",
    "SET TBLPROPERTIES (delta.enableChangeDataFeed = true);"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "71627c5c-51c4-43e2-a10c-bdf34a0b1e87",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "DESCRIBE TABLE EXTENDED silver"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "0c80ecce-4b53-45fc-aa02-6e33a8bee474",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Upsert Data with Delta Lake\n",
    "\n",
    "Here we define upsert logic into the silver table using a streaming read against the bronze table, matching on our unique identifier **`mrn`**.\n",
    "\n",
    "We specify an additional conditional check to ensure that a field in the data has changed before inserting the new record."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "7b8652da-b4e8-46bb-978d-f14e8ddfb778",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def upsert_to_delta(microBatchDF, batchId):\n",
    "    microBatchDF.createOrReplaceTempView(\"updates\")\n",
    "    microBatchDF._jdf.sparkSession().sql(\"\"\"\n",
    "        MERGE INTO silver s\n",
    "        USING updates u\n",
    "        ON s.mrn = u.mrn\n",
    "        WHEN MATCHED AND s.dob <> u.dob OR\n",
    "                         s.sex <> u.sex OR\n",
    "                         s.gender <> u.gender OR\n",
    "                         s.first_name <> u.first_name OR\n",
    "                         s.last_name <> u.last_name OR\n",
    "                         s.street_address <> u.street_address OR\n",
    "                         s.zip <> u.zip OR\n",
    "                         s.city <> u.city OR\n",
    "                         s.state <> u.state OR\n",
    "                         s.updated <> u.updated\n",
    "            THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED\n",
    "            THEN INSERT *\n",
    "    \"\"\")"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "1b2272a0-bcd8-4bf9-be16-9734549df709",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "query = (spark.readStream\n",
    "              .table(\"bronze\")\n",
    "              .writeStream\n",
    "              .foreachBatch(upsert_to_delta)\n",
    "              .outputMode(\"update\")\n",
    "              # .trigger(availableNow=True)\n",
    "              .trigger(processingTime='5 seconds')\n",
    "              .start())\n",
    "\n",
    "DA.block_until_stream_is_ready(query)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "45428ca8-16ad-4e70-b641-4948d4c44da0",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that we have an additional metadata directory nested in our table directory, **`_change_data`**"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "748bef97-5673-421e-8b6f-ba5fe5a2bc37",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "files = dbutils.fs.ls(f\"{DA.paths.user_db}/silver\")\n",
    "display(files)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "9e925093-1273-495b-a173-c755ce580e17",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see this directory also contains parquet files."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "b167277f-cac0-42b6-8815-0d464dcb6d48",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "files = dbutils.fs.ls(f\"{DA.paths.user_db}/silver/_change_data\")\n",
    "display(files)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "39970b2f-03fc-44e9-9206-ee7449b3ec8b",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read the Change Data Feed\n",
    "\n",
    "To pick up the recorded CDC data, we add two options:\n",
    "- **`readChangeData`**\n",
    "- **`startingVersion`** (can use **`startingTimestamp`** instead)\n",
    "\n",
    "Here we'll do a streaming display of just those patients in LA. Note that users with changes have two records present."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "1483bb7a-86b6-4e7d-b174-4a3d81100905",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "cdc_df = (spark.readStream\n",
    "               .format(\"delta\")\n",
    "               .option(\"readChangeData\", True)\n",
    "               .option(\"startingVersion\", 0)\n",
    "               .table(\"silver\"))\n",
    "\n",
    "cdc_la_df = cdc_df.filter(\"city = 'Los Angeles'\")\n",
    "\n",
    "display(cdc_la_df, streamName = \"display_la\")\n",
    "DA.block_until_stream_is_ready(name = \"display_la\")"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "2fd53234-2d4a-4b61-88bb-e8865409961b",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we land another file in our source directory and wait a few seconds, we'll see that we now have captured CDC changes for multiple **`_commit_version`** (change the sort order of the **`_commit_version`** column in the display above to see this)."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "d058f8e0-6ab9-4bb8-8482-07061bfc5209",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "DA.cdc_stream.load()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "56707914-4e6d-467a-82a3-95e9a8e935b3",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gold Table\n",
    "Our gold table will capture all of those patients that have a new address, and record this information alongside 2 timestamps: the time at which this change was made in our source system (currently labeled **`updated`**) and the time this was processed into our silver table (captured by the **`_commit_timestamp`** generated CDC field).\n",
    "\n",
    "Within silver table CDC records:\n",
    "- check for max **`_commit_version`** for each record\n",
    "- if new version and address change, insert to gold table\n",
    "- record **`updated_timestamp`** and **`processed_timestamp`**\n",
    "\n",
    "#### Gold Table Schema\n",
    "| field | type |\n",
    "| --- | --- |\n",
    "| mrn | long |\n",
    "| new_street_address | string |\n",
    "| new_zip | long |\n",
    "| new_city | string |\n",
    "| new_state | string |\n",
    "| old_street_address | string |\n",
    "| old_zip | long |\n",
    "| old_city | string |\n",
    "| old_state | string |\n",
    "| updated_timestamp | timestamp |\n",
    "| processed_timestamp | timestamp |"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "89b2f871-7d48-47a5-b870-5e58dc2b685d",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "CREATE TABLE gold (mrn BIGINT,\n",
    "                   new_street_address STRING,\n",
    "                   new_zip BIGINT,\n",
    "                   new_city STRING,\n",
    "                   new_state STRING,\n",
    "                   old_street_address STRING,\n",
    "                   old_zip BIGINT,\n",
    "                   old_city STRING,\n",
    "                   old_state STRING,\n",
    "                   updated_timestamp TIMESTAMP,\n",
    "                   processed_timestamp TIMESTAMP)\n",
    "USING DELTA\n",
    "LOCATION '${da.paths.working_dir}/gold'"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "04f3c03a-1171-4ceb-af02-22d47584414d",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that we are using a table that has updates written to it as a streaming source! \n",
    "\n",
    "This is a **huge** value add, and something that historically has required extensive workarounds to process correctly."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "261cba40-95cb-4354-a412-7d889c15264f",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "silver_stream_df = (spark.readStream\n",
    "                         .format(\"delta\")\n",
    "                         .option(\"readChangeData\", True)\n",
    "                         .option(\"startingVersion\", 0)\n",
    "                         .table(\"silver\"))"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "2ac5db90-8db1-4028-a830-8cd03163b538",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our **`_change_type`** field lets us easily distinguish valid and invalid records.\n",
    "\n",
    "New valid rows will have the **`update_postimage`** or **`insert`** label.\n",
    "New invalid rows will have the **`update_preimage`** or **`delete`** label. \n",
    "\n",
    "(**NOTE**: We'll demonstrate logic for propagating deletes a little later)\n",
    "\n",
    "In the cell below, we'll define two queries against our streaming source to perform a stream-stream merge on our data."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "92d3aeb5-3a02-41c3-90fb-8ff36f8e8d2a",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "new_df = (silver_stream_df\n",
    "         .filter(F.col(\"_change_type\").isin([\"update_postimage\", \"insert\"]))\n",
    "         .selectExpr(\"mrn\",\n",
    "                     \"street_address AS new_street_address\",\n",
    "                     \"zip AS new_zip\",\n",
    "                     \"city AS new_city\",\n",
    "                     \"state AS new_state\",\n",
    "                     \"updated AS updated_timestamp\",\n",
    "                     \"_commit_timestamp AS processed_timestamp\"))\n",
    "                                                                                         \n",
    "old_df = (silver_stream_df\n",
    "         .filter(F.col(\"_change_type\").isin([\"update_preimage\"]))\n",
    "         .selectExpr(\"mrn\",\n",
    "                     \"street_address AS old_street_address\",\n",
    "                     \"zip AS old_zip\",\n",
    "                     \"city AS old_city\",\n",
    "                     \"state AS old_state\",\n",
    "                     \"_commit_timestamp AS processed_timestamp\"))"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "57780754-b6e6-47d2-8ec2-1328a5598d6c",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Assuming that we have properly deduplicated our data to ensure that only a single record for our **`mrn`** can be processed to our silver table, **`mrn`** and **`_commit_timestamp`** (aliased to **`processed_timestamp`** here) serve as a unique composite key.\n",
    "\n",
    "Our join will allow us to match up the current and previous states of our data to track all changes.\n",
    "\n",
    "This table could drive further downstream processes, such as triggering confirmation emails or automatic mailings for patients with updated addresses.\n",
    "\n",
    "Our CDC data arrives as a stream, so only newly changed data at the silver level will be processed. Therefore, we can write to our gold table in append mode and maintain the grain of our data."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "3f6a50dc-de15-472c-85b0-ef83cfb8ee6e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "query = (new_df.withWatermark(\"processed_timestamp\", \"3 minutes\")\n",
    "               .join(old_df, [\"mrn\", \"processed_timestamp\"], \"left\")\n",
    "               .filter(\"new_street_address <> old_street_address OR old_street_address IS NULL\")\n",
    "               .writeStream\n",
    "               .outputMode(\"append\")\n",
    "               #.trigger(availableNow=True)\n",
    "               .trigger(processingTime=\"5 seconds\")\n",
    "               .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/gold\")\n",
    "               .table(\"gold\"))\n",
    "\n",
    "DA.block_until_stream_is_ready(query)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "d0be0305-717d-4503-80e2-d95e248951f8",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note the number of rows in our gold table."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "58002d29-b52f-45ec-9171-19f0759e94a8",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT * FROM gold"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "7fadde3f-9e49-4293-aa0a-6b24bafe9041",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we land a new raw file and wait a few seconds, we can see that all of our changes have propagated through our pipeline.\n",
    "\n",
    "(This assumes you're using **`processingTime`** instead of trigger-once or trigger-available-now processing. Scroll up to the gold table streaming write to wait for a new peak in the processing rate to know your data has arrived.)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "f750c2ae-d7c2-4af4-b706-4c2d60a5cf06",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "DA.cdc_stream.load()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "8ae6e4a6-b414-4ebb-baa9-e53931d427e8",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "You should be able to see a jump in the number of records in your gold table."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "366bc953-0d32-439e-9063-a0c1e7f5c8b1",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT * FROM gold"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "40b294bf-567e-4866-bc0e-d2994fa9e135",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make sure to run the following cell to stop all active streams."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "d1b35c0a-145a-4c97-9b1b-90b2c191b834",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for stream in spark.streams.active:\n",
    "    stream.stop()\n",
    "    stream.awaitTermination()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "1c961795-20da-43ae-8d1b-93eb6cb6f34b",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Propagating Deletes\n",
    "\n",
    "While some use cases may require processing deletes alongside updates and inserts, the most important delete requests are those that allow companies to maintain compliance with privacy regulations such as GDPR and CCPA. Most companies have stated SLAs around how long these requests will take to process, but for various reasons, these are often handled in pipelines separate from their core ETL.\n",
    "\n",
    "Here, we should a single user being deleted from our **`silver`** table."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "2bfa862b-559f-4735-ac03-33c1f6d7fae8",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "DELETE FROM silver WHERE mrn = 14125426"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "c4b56ecf-a0a1-42db-8df4-f80a4887363a",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "As expected, when we try to locate this user in our **`silver`** table, we'll get no result."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "64856a41-e530-4ab3-a0ba-5299c42d5172",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT * FROM silver WHERE mrn = 14125426"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "fd3a0938-f01a-4da8-8998-bb90b184c181",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "This change has been captured in our Change Data Feed."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "0e44c024-ee50-467f-990d-e530e73d8a0b",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM table_changes(\"silver\", 0)\n",
    "WHERE mrn = 14125426\n",
    "ORDER BY _commit_version"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "742041c8-b922-4ebf-8c8f-f4e3213b8a8c",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because we have a record of this delete action, we can define logic that propagates deletes to our **`gold`** table."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "7e055454-1b46-48dd-a6fd-14f95c282885",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "WITH deletes AS (\n",
    "  SELECT mrn\n",
    "  FROM table_changes(\"silver\", 0)\n",
    "  WHERE _change_type='delete'\n",
    ")\n",
    "\n",
    "MERGE INTO gold g\n",
    "USING deletes d\n",
    "ON d.mrn=g.mrn\n",
    "WHEN MATCHED\n",
    "  THEN DELETE"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "1e271ee5-1e91-43ed-9aca-1ce990222c2b",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "This drastically simplifies deleting user data, and allows the keys and logic used in your ETL to also be used for propagating delete requests."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "c0364eea-9cff-4529-9841-47f401404e51",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT * FROM gold WHERE mrn = 14125426"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "0a902c23-e663-441b-806d-f074a650bf38",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the following cell to delete the tables and files associated with this lesson."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "5f399fd9-8b22-4b97-97ca-3c4cc9d75019",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "DA.cleanup()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "5d92d326-b9eb-4ef6-b784-79ef8e431e95",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "notebookName": "ADE 6.1 - Processing Records from Change Data Feed",
   "dashboards": [],
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "language": "python",
   "widgets": {},
   "notebookOrigID": 4185566132529617
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}