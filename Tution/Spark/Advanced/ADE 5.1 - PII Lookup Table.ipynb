{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Creating a Pseudonymized PII Lookup Table\n",
    "\n",
    "In this lesson we'll create a pseudonymized key for storing potentially sensitive user data.\n",
    "\n",
    "Our approach in this notebook is fairly straightforward; some industries may require more elaborate de-identification to guarantee privacy.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/ade/ADE_arch_user_lookup.png\" width=\"60%\" />\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lesson, students will be able to:\n",
    "- Describe the purpose of \"salting\" before hashing\n",
    "- Apply salted hashing to sensitive data for pseudonymization\n",
    "- Use Auto Loader to process incremental inserts"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "8417fdb4-dad0-4567-b1f7-719718f4d501",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "Begin by running the following cell to set up relevant databases and paths."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "7c1315e6-991d-4e60-a1a3-fd071afea873",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%run ../Includes/Classroom-Setup-6.1"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "caf34b37-db55-48d9-aa50-033d4f761449",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Auto Load Bronze Data\n",
    "\n",
    "The following cell defines and executes logic to incrementally ingest data into the **`registered_users`** table with Auto Loader.\n",
    "\n",
    "This logic is currently configured to process a single file each time a batch is triggered (currently every 10 seconds).\n",
    "\n",
    "Executing this cell will start an always-on stream that slowly ingests new files as they arrive."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "7fa1a897-c710-4d87-a5dc-cf13ca80dcda",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "query = (spark.readStream\n",
    "              .schema(\"device_id LONG, mac_address STRING, registration_timestamp DOUBLE, user_id LONG\")\n",
    "              .format(\"cloudFiles\")\n",
    "              .option(\"cloudFiles.format\", \"json\")\n",
    "              .option(\"cloudFiles.maxFilesPerTrigger\", 1)\n",
    "              .load(DA.paths.raw_user_reg)\n",
    "              .writeStream\n",
    "              .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/registered_users\")\n",
    "              .trigger(processingTime=\"10 seconds\")\n",
    "              .table(\"registered_users\"))\n",
    "\n",
    "DA.block_until_stream_is_ready(query)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "c6ff814d-6355-41b9-85dc-60140e94f358",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before moving on with this lesson, we need to:\n",
    "1. Stop the existing stream\n",
    "2. Drop the table we created\n",
    "3. Clear the checkpoint directory"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "a645189b-364c-4d3a-8354-c4f683f9ddfe",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "query.stop()\n",
    "query.awaitTermination()\n",
    "spark.sql(\"DROP TABLE IF EXISTS registered_users\")\n",
    "dbutils.fs.rm(f\"{DA.paths.checkpoints}/registered_users\", True)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "327bdeb6-9862-4a99-9ae3-0ef6361dea37",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the cell below to refactor the above query into a function that processes new files as a single incremental triggered batch.\n",
    "\n",
    "To do this:\n",
    "* Remove the option limiting the amount of files processed per trigger (this is ignored when executing a batch anyway)\n",
    "* Change the trigger type to \"availableNow\"\n",
    "* Make sure to add **`.awaitTermination()`** to the end of your query to block execution of the next cell until the batch has completed"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "5101e63c-9d3e-4990-a1d4-c4545b51d521",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO\n",
    "def ingest_user_reg():\n",
    "    pass"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "e2ae39a6-ada3-41d3-86ad-ff699eee6118",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use your function below.\n",
    "\n",
    "**NOTE**: This triggered batch will automatically cause our always-on stream to fail because the same checkpoint is used; default behavior will allow the newer query to succeed and error the older query."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "65e926a4-d993-40ec-a3ee-d1ffd46a9dc6",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ingest_user_reg()\n",
    "display(spark.table(\"registered_users\"))"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "ac06974d-d08d-4545-b157-26616315600e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another custom class was initialized in the setup script to land a new batch of data in our source directory. \n",
    "\n",
    "Execute the following cell and note the difference in the total number of rows in our tables."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "82c1da72-ce15-4ad0-805a-457ce56a810c",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "DA.user_reg_stream.load()\n",
    "\n",
    "ingest_user_reg()\n",
    "display(spark.table(\"registered_users\"))"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "767aba53-f70a-422a-ad38-bc995fddaa07",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add a Salt to Natural Key\n",
    "We'll start by defining a salt, here in plain text. We'll combine this salt with our natural key, **`user_id`**, before applying a hash function to generate a pseudonymized key.\n",
    "\n",
    "Salting before hashing is very important as it makes dictionary attacks to reverse the hash much more expensive.  To demonstrate, try reversing the following SHA-256 hash of `Secrets123` by searching it's hash using Google: `FCF730B6D95236ECD3C9FC2D92D7B6B2BB061514961AEC041D6C7A7192F592E4` bringing you to this [link](https://hashtoolkit.com/decrypt-sha256-hash/fcf730b6d95236ecd3c9fc2d92d7b6b2bb061514961aec041d6c7a7192f592e4).  Next, try hashing `Secrets123:BEANS` [here](https://passwordsgenerator.net/sha256-hash-generator/) and perform a similar search.  Notice how adding the salt `BEANS` improved the security.\n",
    "\n",
    "For greater security, we could upload the salt as a secret using the Databricks <a href=\"https://docs.databricks.com/security/secrets/secrets.html\" target=\"_blank\">Secrets</a> utility."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "8cee9426-b563-4b2b-b3fb-b4bab520813a",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "salt = 'BEANS'\n",
    "spark.conf.set(\"da.salt\", salt)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "f359a0d4-ec48-4fdd-aa08-bdf3f5a4723c",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "# # If using the Databricks secrets store, here's how you'd read it...\n",
    "# salt = dbutils.secrets.get(scope=\"DA-ADE3.03\", key=\"salt\")\n",
    "# salt"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "16c98a08-80a5-4a93-a649-cbf122c3003e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preview what your new key will look like."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "3a3c69f5-ef60-4022-85bb-f2de2c56eb91",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "SELECT *, sha2(concat(user_id,\"${da.salt}\"), 256) AS alt_id\n",
    "FROM registered_users"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "c0d4b2d8-0c2f-4f21-af39-84fe013667db",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Register a SQL UDF\n",
    "\n",
    "Create a SQL user-defined function to register this logic to the current database under the name **`salted_hash`**. \n",
    "\n",
    "This will allow this logic to be called by any user with appropriate permissions on this function. \n",
    "\n",
    "Make sure your UDF accepts one parameter: a **`String`** and it should return a **`STRING`**.  You can access the configured salt value by using the expression `\"${da.salt}\"`.\n",
    "\n",
    "For more information, see the <a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/sql-ref-syntax-ddl-create-sql-function.html\" target=\"_blank\">CREATE FUNCTION</a> method from the SQL UDFs docs."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "aa849d91-51c8-4d32-aee3-26dc51085494",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "-- TODO\n",
    "CREATE FUNCTION salted_hash <FILL-IN>"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "404daaab-5d18-4206-b6e7-b856d4b4d173",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "If your SQL UDF is defined correctly, the assert statement below should run without error."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "d82b20d6-144e-4a61-9167-0e9e01cc4b65",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Check your work\n",
    "set_a = spark.sql(f\"SELECT sha2(concat('USER123', '{salt}'), 256) alt_id\").collect()\n",
    "set_b = spark.sql(\"SELECT salted_hash('USER123') alt_id\").collect()\n",
    "assert set_a == set_b, \"The 'salted_hash' function is returning the wrong result.\"\n",
    "print(\"All tests passed.\")"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "6803659d-e4a9-46f5-9c69-d5603ef28f0e",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that it is theoretically possible to link the original key and pseudo-ID if the hash function and the salt are known.\n",
    "\n",
    "Here, we use this method to add a layer of obfuscation; in production, you may wish to have a much more sophisticated hashing method."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "df6c9be4-6437-483b-a9a3-bbb0514ccf40",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Register Target Table\n",
    "The logic below creates the **`user_lookup`** table.\n",
    "\n",
    "Here we're just creating our **`user_lookup`** table. In the next notebook, we'll use this pseudo-ID as the sole link to user PII.\n",
    "\n",
    "By controlling access to the link between our **`alt_id`** and other natural keys, we'll be able to prevent linking PII to other user data throughout our system."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "dcd8ca8a-8acc-4fd4-ae05-ec4a2a3e47fe",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS user_lookup (alt_id string, device_id long, mac_address string, user_id long)\n",
    "USING DELTA \n",
    "LOCATION '${da.paths.working_dir}/user_lookup'"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "3096cda5-13d0-4b10-80e9-28f47738b7ee",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define a Function for Processing Incremental Batches\n",
    "\n",
    "The cell below includes the setting for the correct checkpoint path.\n",
    "\n",
    "Define a function to apply the SQL UDF registered above to create your **`alt_id`** to the **`user_id`** from the **`registered_users`** table.\n",
    "\n",
    "Make sure you include all the necessary columns for the target **`user_lookup`** table. Configure your logic to execute as a triggered incremental batch."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "c73ef7ad-3d8e-4eeb-8af3-2c10aa9f699f",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO\n",
    "def load_user_lookup():\n",
    "    (spark.readStream\n",
    "        <FILL-IN>\n",
    "        .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/user_lookup\")\n",
    "        <FILL-IN>\n",
    "    )"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "cbc5512c-d64c-4f22-88bb-a5c2101c1ead",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use your method below and display the results."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "ceb18557-94c3-445c-b89a-4a5fbcc6a340",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "load_user_lookup()\n",
    "\n",
    "display(spark.table(\"user_lookup\"))"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "4586e625-e976-4bd9-b2ff-b3414b6429ac",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Process another batch of data below to confirm that the incremental processing is working through the entire pipeline."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "1fb9dacb-719f-472b-9aa4-ce4caa255b34",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "DA.user_reg_stream.load()\n",
    "\n",
    "ingest_user_reg()\n",
    "load_user_lookup()\n",
    "\n",
    "display(spark.table(\"user_lookup\"))"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "924b785d-0edf-4f4f-866c-c44bdc68a47a",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "The code below ingests all the remaining records to put 100 total users in the **`user_lookup`** table."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "dfd7bb89-af38-4480-91f0-836b45f99ef1",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "DA.user_reg_stream.load(continuous=True)\n",
    "\n",
    "ingest_user_reg()\n",
    "load_user_lookup()\n",
    "\n",
    "display(spark.table(\"user_lookup\"))"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "3f72da09-562c-46f4-be81-bf18aafd84e0",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll apply this same hashing method to process and store PII data in the next lesson."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "9e99f718-0f3a-42ed-a53d-541bea8a02ba",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the following cell to delete the tables and files associated with this lesson."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "772de146-8d61-4a67-bf85-3b675e3f1285",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "DA.cleanup()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "35c8616a-8394-4d63-acd3-35c5de23dba0",
     "inputWidgets": {},
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": 0
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "notebookName": "ADE 5.1 - PII Lookup Table",
   "dashboards": [],
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "language": "python",
   "widgets": {},
   "notebookOrigID": 4185566132530452
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}